{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54d16df7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:48:50.521493Z",
     "iopub.status.busy": "2025-08-18T14:48:50.521298Z",
     "iopub.status.idle": "2025-08-18T14:51:52.702598Z",
     "shell.execute_reply": "2025-08-18T14:51:52.701995Z"
    },
    "papermill": {
     "duration": 182.186465,
     "end_time": "2025-08-18T14:51:52.703689",
     "exception": false,
     "start_time": "2025-08-18T14:48:50.517224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset copied to working directory.\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0189.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0190.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0191.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0192.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0193.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0194.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0195.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0196.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0197.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0198.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0199.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0200.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0201.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0202.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0203.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0204.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0205.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0206.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0207.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0208.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0209.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0210.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0211.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0212.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0213.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0214.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0215.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0216.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0217.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0218.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0219.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0220.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0221.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0222.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0223.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0224.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0225.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0226.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0227.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0228.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0229.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0230.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0231.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0232.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0233.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0234.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0235.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0236.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0237.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0238.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0239.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0240.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0241.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0242.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0243.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0244.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0245.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0246.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0247.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0248.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0249.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0250.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0251.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0252.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0253.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0254.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0255.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0256.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0257.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0258.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0259.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0260.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0261.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0262.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0263.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0264.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0265.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0266.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0267.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0268.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0269.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0270.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0271.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0272.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0273.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0274.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0275.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0276.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0277.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0278.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0279.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0280.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0281.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0282.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0283.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0284.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0285.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0286.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0287.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0288.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0289.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0290.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0291.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0292.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0293.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0294.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0295.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0296.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0297.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0298.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0299.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0300.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0301.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0302.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0303.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0304.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0305.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0306.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0307.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0308.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0309.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0310.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0311.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0312.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0313.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0314.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0315.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0316.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0317.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0318.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0319.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0320.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0321.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0322.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0323.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0324.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0325.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0326.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0327.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0328.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0329.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0330.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0331.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0332.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0333.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0334.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0335.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0336.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0337.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0338.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0339.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0340.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0341.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0342.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0343.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0344.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0345.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0346.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0347.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0348.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0349.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0350.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0351.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0352.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0353.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0354.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0355.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0356.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0357.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0358.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0359.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0360.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0361.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0362.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0363.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0364.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0365.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0366.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0367.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0368.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0369.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0370.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0371.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0372.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0373.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0374.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0375.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/cover/cover_0376.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0182.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0183.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0184.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0185.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0186.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0187.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0188.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0189.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0190.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0191.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0192.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0193.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0194.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0195.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0196.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0197.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0198.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0199.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0200.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0201.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0202.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0203.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0204.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0205.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0206.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0207.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0208.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0209.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0210.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0211.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0212.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0213.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0214.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0215.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0216.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0217.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0218.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0219.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0220.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0221.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0222.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0223.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0224.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0225.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0226.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0227.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0228.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0229.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0230.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0231.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0232.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0233.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0234.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0235.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0236.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0237.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0238.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0239.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0240.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0241.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0242.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0243.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0244.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0245.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0246.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0247.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0248.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0249.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0250.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0251.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0252.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0253.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0254.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0255.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0256.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0257.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0258.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0259.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0260.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0261.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0262.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0263.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0264.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0265.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0266.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0267.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0268.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0269.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0270.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0271.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0272.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0273.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0274.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0275.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0276.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0277.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0278.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0279.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0280.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0281.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0282.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0283.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0284.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0285.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0286.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0287.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0288.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0289.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0290.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0291.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0292.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0293.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0294.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0295.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0296.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0297.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0298.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0299.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0300.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0301.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0302.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0303.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0304.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0305.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0306.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0307.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0308.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0309.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0310.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0311.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0312.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0313.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0314.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0315.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0316.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0317.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0318.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0319.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0320.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0321.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0322.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0323.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0324.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0325.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0326.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0327.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0328.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0329.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0330.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0331.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0332.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0333.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0334.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0335.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0336.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0337.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0338.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0339.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0340.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0341.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0342.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0343.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0344.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0345.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0346.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0347.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0348.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0349.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0350.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0351.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0352.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0353.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0354.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0355.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0356.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0357.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0358.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0359.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0360.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0361.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/hook/hook_0362.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0193.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0194.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0195.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0196.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0197.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0198.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0199.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0200.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0201.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0202.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0203.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0204.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0205.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0206.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0207.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0208.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0209.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0210.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0211.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0212.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0213.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0214.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0215.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0216.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0217.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0218.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0219.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0220.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0221.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0222.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0223.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0224.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0225.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0226.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0227.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0228.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0229.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0230.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0231.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0232.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0233.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0234.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0235.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0236.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0237.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0238.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0239.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0240.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0241.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0242.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0243.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0244.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0245.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0246.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0247.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0248.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0249.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0250.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0251.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0252.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0253.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0254.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0255.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0256.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0257.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0258.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0259.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0260.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0261.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0262.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0263.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0264.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0265.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0266.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0267.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0268.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0269.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0270.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0271.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0272.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0273.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0274.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0275.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0276.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0277.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0278.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0279.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0280.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0281.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0282.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0283.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0284.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0285.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0286.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0287.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0288.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0289.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0290.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0291.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0292.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0293.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0294.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0295.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0296.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0297.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0298.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0299.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0300.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0301.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0302.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0303.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0304.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0305.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0306.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0307.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0308.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0309.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0310.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0311.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0312.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0313.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0314.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0315.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0316.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0317.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0318.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0319.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0320.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0321.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0322.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0323.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0324.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0325.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0326.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0327.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0328.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0329.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0330.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0331.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0332.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0333.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0334.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0335.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0336.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0337.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0338.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0339.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0340.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0341.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0342.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0343.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0344.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0345.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0346.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0347.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0348.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0349.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0350.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0351.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0352.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0353.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0354.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0355.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0356.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0357.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0358.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0359.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0360.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0361.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0362.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0363.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0364.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0365.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0366.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0367.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0368.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0369.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0370.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0371.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0372.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0373.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0374.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0375.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0376.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0377.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0378.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0379.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0380.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0381.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0382.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0383.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/defense/defense_0384.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0194.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0195.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0196.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0197.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0198.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0199.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0200.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0201.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0202.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0203.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0204.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0205.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0206.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0207.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0208.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0209.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0210.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0211.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0212.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0213.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0214.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0215.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0216.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0217.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0218.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0219.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0220.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0221.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0222.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0223.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0224.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0225.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0226.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0227.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0228.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0229.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0230.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0231.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0232.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0233.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0234.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0235.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0236.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0237.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0238.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0239.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0240.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0241.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0242.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0243.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0244.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0245.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0246.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0247.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0248.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0249.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0250.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0251.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0252.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0253.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0254.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0255.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0256.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0257.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0258.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0259.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0260.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0261.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0262.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0263.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0264.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0265.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0266.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0267.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0268.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0269.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0270.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0271.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0272.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0273.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0274.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0275.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0276.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0277.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0278.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0279.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0280.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0281.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0282.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0283.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0284.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0285.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0286.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0287.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0288.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0289.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0290.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0291.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0292.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0293.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0294.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0295.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0296.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0297.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0298.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0299.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0300.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0301.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0302.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0303.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0304.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0305.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0306.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0307.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0308.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0309.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0310.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0311.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0312.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0313.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0314.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0315.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0316.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0317.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0318.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0319.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0320.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0321.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0322.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0323.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0324.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0325.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0326.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0327.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0328.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0329.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0330.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0331.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0332.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0333.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0334.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0335.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0336.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0337.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0338.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0339.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0340.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0341.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0342.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0343.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0344.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0345.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0346.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0347.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0348.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0349.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0350.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0351.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0352.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0353.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0354.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0355.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0356.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0357.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0358.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0359.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0360.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0361.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0362.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0363.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0364.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0365.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0366.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0367.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0368.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0369.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0370.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0371.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0372.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0373.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0374.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0375.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0376.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0377.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0378.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0379.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0380.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0381.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0382.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0383.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0384.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0385.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/straight/straight_0386.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0183.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0184.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0185.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0186.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0187.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0188.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0189.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0190.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0191.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0192.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0193.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0194.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0195.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0196.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0197.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0198.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0199.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0200.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0201.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0202.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0203.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0204.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0205.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0206.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0207.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0208.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0209.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0210.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0211.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0212.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0213.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0214.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0215.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0216.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0217.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0218.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0219.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0220.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0221.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0222.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0223.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0224.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0225.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0226.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0227.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0228.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0229.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0230.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0231.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0232.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0233.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0234.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0235.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0236.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0237.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0238.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0239.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0240.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0241.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0242.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0243.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0244.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0245.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0246.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0247.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0248.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0249.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0250.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0251.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0252.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0253.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0254.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0255.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0256.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0257.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0258.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0259.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0260.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0261.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0262.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0263.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0264.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0265.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0266.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0267.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0268.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0269.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0270.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0271.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0272.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0273.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0274.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0275.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0276.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0277.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0278.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0279.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0280.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0281.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0282.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0283.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0284.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0285.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0286.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0287.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0288.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0289.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0290.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0291.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0292.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0293.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0294.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0295.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0296.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0297.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0298.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0299.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0300.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0301.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0302.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0303.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0304.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0305.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0306.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0307.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0308.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0309.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0310.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0311.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0312.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0313.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0314.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0315.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0316.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0317.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0318.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0319.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0320.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0321.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0322.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0323.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0324.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0325.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0326.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0327.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0328.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0329.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0330.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0331.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0332.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0333.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0334.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0335.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0336.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0337.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0338.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0339.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0340.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0341.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0342.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0343.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0344.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0345.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0346.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0347.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0348.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0349.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0350.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0351.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0352.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0353.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0354.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0355.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0356.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0357.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0358.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0359.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0360.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0361.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0362.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0363.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/late_cut/late_cut_0364.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0201.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0202.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0203.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0204.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0205.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0206.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0207.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0208.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0209.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0210.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0211.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0212.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0213.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0214.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0215.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0216.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0217.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0218.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0219.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0220.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0221.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0222.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0223.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0224.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0225.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0226.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0227.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0228.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0229.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0230.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0231.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0232.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0233.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0234.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0235.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0236.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0237.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0238.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0239.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0240.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0241.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0242.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0243.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0244.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0245.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0246.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0247.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0248.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0249.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0250.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0251.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0252.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0253.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0254.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0255.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0256.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0257.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0258.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0259.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0260.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0261.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0262.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0263.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0264.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0265.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0266.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0267.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0268.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0269.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0270.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0271.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0272.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0273.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0274.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0275.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0276.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0277.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0278.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0279.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0280.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0281.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0282.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0283.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0284.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0285.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0286.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0287.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0288.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0289.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0290.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0291.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0292.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0293.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0294.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0295.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0296.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0297.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0298.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0299.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0300.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0301.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0302.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0303.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0304.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0305.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0306.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0307.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0308.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0309.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0310.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0311.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0312.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0313.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0314.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0315.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0316.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0317.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0318.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0319.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0320.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0321.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0322.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0323.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0324.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0325.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0326.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0327.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0328.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0329.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0330.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0331.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0332.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0333.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0334.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0335.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0336.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0337.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0338.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0339.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0340.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0341.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0342.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0343.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0344.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0345.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0346.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0347.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0348.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0349.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0350.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0351.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0352.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0353.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0354.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0355.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0356.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0357.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0358.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0359.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0360.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0361.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0362.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0363.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0364.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0365.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0366.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0367.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0368.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0369.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0370.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0371.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0372.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0373.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0374.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0375.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0376.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0377.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0378.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0379.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0380.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0381.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0382.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0383.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0384.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0385.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0386.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0387.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0388.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0389.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0390.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0391.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0392.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0393.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0394.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0395.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0396.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0397.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0398.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0399.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/square_cut/square_cut_0400.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0180.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0181.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0182.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0183.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0184.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0185.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0186.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0187.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0188.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0189.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0190.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0191.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0192.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0193.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0194.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0195.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0196.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0197.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0198.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0199.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0200.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0201.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0202.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0203.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0204.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0205.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0206.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0207.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0208.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0209.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0210.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0211.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0212.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0213.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0214.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0215.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0216.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0217.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0218.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0219.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0220.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0221.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0222.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0223.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0224.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0225.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0226.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0227.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0228.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0229.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0230.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0231.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0232.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0233.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0234.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0235.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0236.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0237.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0238.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0239.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0240.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0241.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0242.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0243.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0244.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0245.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0246.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0247.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0248.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0249.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0250.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0251.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0252.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0253.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0254.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0255.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0256.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0257.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0258.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0259.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0260.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0261.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0262.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0263.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0264.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0265.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0266.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0267.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0268.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0269.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0270.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0271.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0272.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0273.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0274.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0275.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0276.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0277.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0278.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0279.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0280.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0281.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0282.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0283.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0284.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0285.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0286.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0287.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0288.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0289.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0290.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0291.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0292.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0293.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0294.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0295.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0296.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0297.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0298.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0299.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0300.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0301.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0302.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0303.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0304.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0305.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0306.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0307.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0308.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0309.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0310.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0311.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0312.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0313.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0314.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0315.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0316.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0317.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0318.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0319.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0320.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0321.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0322.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0323.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0324.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0325.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0326.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0327.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0328.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0329.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0330.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0331.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0332.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0333.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0334.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0335.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0336.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0337.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0338.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0339.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0340.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0341.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0342.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0343.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0344.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0345.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0346.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0347.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0348.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0349.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0350.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0351.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0352.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0353.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0354.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0355.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0356.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0357.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/pull/pull_0358.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0199.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0200.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0201.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0202.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0203.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0204.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0205.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0206.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0207.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0208.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0209.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0210.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0211.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0212.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0213.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0214.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0215.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0216.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0217.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0218.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0219.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0220.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0221.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0222.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0223.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0224.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0225.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0226.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0227.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0228.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0229.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0230.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0231.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0232.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0233.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0234.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0235.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0236.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0237.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0238.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0239.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0240.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0241.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0242.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0243.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0244.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0245.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0246.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0247.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0248.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0249.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0250.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0251.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0252.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0253.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0254.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0255.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0256.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0257.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0258.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0259.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0260.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0261.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0262.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0263.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0264.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0265.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0266.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0267.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0268.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0269.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0270.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0271.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0272.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0273.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0274.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0275.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0276.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0277.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0278.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0279.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0280.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0281.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0282.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0283.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0284.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0285.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0286.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0287.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0288.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0289.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0290.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0291.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0292.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0293.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0294.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0295.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0296.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0297.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0298.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0299.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0300.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0301.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0302.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0303.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0304.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0305.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0306.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0307.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0308.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0309.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0310.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0311.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0312.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0313.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0314.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0315.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0316.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0317.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0318.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0319.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0320.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0321.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0322.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0323.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0324.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0325.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0326.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0327.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0328.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0329.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0330.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0331.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0332.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0333.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0334.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0335.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0336.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0337.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0338.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0339.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0340.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0341.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0342.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0343.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0344.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0345.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0346.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0347.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0348.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0349.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0350.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0351.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0352.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0353.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0354.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0355.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0356.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0357.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0358.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0359.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0360.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0361.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0362.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0363.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0364.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0365.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0366.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0367.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0368.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0369.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0370.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0371.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0372.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0373.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0374.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0375.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0376.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0377.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0378.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0379.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0380.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0381.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0382.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0383.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0384.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0385.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0386.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0387.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0388.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0389.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0390.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0391.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0392.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0393.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0394.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0395.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/lofted/lofted_0396.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0195.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0196.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0197.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0198.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0199.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0200.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0201.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0202.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0203.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0204.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0205.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0206.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0207.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0208.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0209.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0210.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0211.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0212.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0213.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0214.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0215.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0216.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0217.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0218.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0219.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0220.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0221.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0222.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0223.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0224.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0225.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0226.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0227.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0228.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0229.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0230.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0231.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0232.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0233.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0234.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0235.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0236.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0237.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0238.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0239.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0240.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0241.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0242.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0243.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0244.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0245.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0246.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0247.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0248.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0249.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0250.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0251.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0252.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0253.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0254.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0255.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0256.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0257.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0258.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0259.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0260.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0261.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0262.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0263.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0264.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0265.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0266.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0267.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0268.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0269.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0270.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0271.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0272.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0273.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0274.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0275.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0276.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0277.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0278.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0279.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0280.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0281.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0282.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0283.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0284.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0285.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0286.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0287.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0288.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0289.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0290.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0291.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0292.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0293.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0294.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0295.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0296.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0297.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0298.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0299.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0300.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0301.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0302.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0303.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0304.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0305.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0306.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0307.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0308.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0309.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0310.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0311.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0312.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0313.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0314.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0315.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0316.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0317.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0318.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0319.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0320.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0321.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0322.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0323.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0324.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0325.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0326.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0327.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0328.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0329.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0330.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0331.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0332.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0333.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0334.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0335.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0336.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0337.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0338.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0339.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0340.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0341.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0342.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0343.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0344.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0345.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0346.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0347.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0348.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0349.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0350.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0351.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0352.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0353.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0354.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0355.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0356.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0357.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0358.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0359.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0360.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0361.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0362.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0363.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0364.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0365.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0366.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0367.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0368.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0369.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0370.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0371.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0372.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0373.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0374.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0375.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0376.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0377.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0378.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0379.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0380.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0381.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0382.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0383.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0384.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0385.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0386.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0387.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/sweep/sweep_0388.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0182.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0183.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0184.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0185.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0186.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0187.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0188.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0189.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0190.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0191.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0192.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0193.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0194.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0195.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0196.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0197.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0198.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0199.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0200.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0201.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0202.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0203.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0204.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0205.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0206.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0207.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0208.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0209.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0210.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0211.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0212.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0213.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0214.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0215.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0216.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0217.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0218.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0219.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0220.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0221.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0222.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0223.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0224.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0225.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0226.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0227.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0228.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0229.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0230.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0231.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0232.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0233.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0234.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0235.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0236.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0237.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0238.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0239.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0240.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0241.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0242.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0243.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0244.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0245.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0246.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0247.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0248.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0249.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0250.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0251.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0252.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0253.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0254.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0255.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0256.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0257.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0258.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0259.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0260.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0261.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0262.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0263.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0264.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0265.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0266.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0267.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0268.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0269.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0270.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0271.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0272.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0273.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0274.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0275.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0276.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0277.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0278.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0279.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0280.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0281.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0282.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0283.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0284.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0285.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0286.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0287.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0288.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0289.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0290.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0291.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0292.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0293.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0294.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0295.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0296.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0297.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0298.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0299.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0300.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0301.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0302.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0303.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0304.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0305.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0306.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0307.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0308.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0309.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0310.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0311.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0312.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0313.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0314.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0315.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0316.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0317.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0318.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0319.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0320.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0321.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0322.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0323.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0324.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0325.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0326.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0327.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0328.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0329.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0330.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0331.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0332.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0333.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0334.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0335.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0336.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0337.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0338.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0339.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0340.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0341.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0342.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0343.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0344.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0345.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0346.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0347.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0348.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0349.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0350.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0351.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0352.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0353.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0354.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0355.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0356.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0357.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0358.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0359.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0360.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0361.mp4\n",
      "Saved flipped: /kaggle/working/augmented_dataset_256p/flick/flick_0362.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import cv2\n",
    "import shutil\n",
    "\n",
    "def extract_index(name):\n",
    "    match = re.search(r\"_(\\d+)\\.mp4$\", name)\n",
    "    return int(match.group(1)) if match else -1\n",
    "\n",
    "input_root = \"/kaggle/input/cricshot10/dataset_256p\"\n",
    "output_root = \"/kaggle/working/augmented_dataset_256p\"\n",
    "\n",
    "# Step 1: Copy original dataset to /kaggle/working\n",
    "if not os.path.exists(output_root):\n",
    "    shutil.copytree(input_root, output_root)\n",
    "    print(\"Original dataset copied to working directory.\")\n",
    "\n",
    "# Step 2: Flip and save with continued numbering\n",
    "for class_name in os.listdir(output_root):\n",
    "    class_dir = os.path.join(output_root, class_name)\n",
    "    if not os.path.isdir(class_dir):\n",
    "        continue\n",
    "\n",
    "    video_files = [f for f in os.listdir(class_dir) if f.endswith(\".mp4\")]\n",
    "    max_index = max([extract_index(f) for f in video_files])\n",
    "\n",
    "    next_index = max_index + 1\n",
    "\n",
    "    for file in video_files:\n",
    "        video_path = os.path.join(class_dir, file)\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "        new_filename = f\"{class_name}_{next_index:04d}.mp4\"\n",
    "        new_path = os.path.join(class_dir, new_filename)\n",
    "\n",
    "        out = cv2.VideoWriter(new_path, fourcc, fps, (width, height))\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            flipped_frame = cv2.flip(frame, 1)\n",
    "            out.write(flipped_frame)\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "\n",
    "        print(f\"Saved flipped: {new_path}\")\n",
    "        next_index += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "905981f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:51:52.761704Z",
     "iopub.status.busy": "2025-08-18T14:51:52.761071Z",
     "iopub.status.idle": "2025-08-18T14:51:59.161096Z",
     "shell.execute_reply": "2025-08-18T14:51:59.160299Z"
    },
    "papermill": {
     "duration": 6.430769,
     "end_time": "2025-08-18T14:51:59.162377",
     "exception": false,
     "start_time": "2025-08-18T14:51:52.731608",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting split-folders\r\n",
      "  Downloading split_folders-0.5.1-py3-none-any.whl.metadata (6.2 kB)\r\n",
      "Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\r\n",
      "Installing collected packages: split-folders\r\n",
      "Successfully installed split-folders-0.5.1\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 3776 files [00:01, 3005.83 files/s]\n"
     ]
    }
   ],
   "source": [
    "%pip install split-folders\n",
    "import splitfolders\n",
    "splitfolders.ratio(input=\"/kaggle/working/augmented_dataset_256p/\", output=\"/kaggle/working/split_data\", seed=1337, ratio=(.70, .15, .15), group_prefix=None, move=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5898ef8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:51:59.220343Z",
     "iopub.status.busy": "2025-08-18T14:51:59.220127Z",
     "iopub.status.idle": "2025-08-18T14:54:13.016523Z",
     "shell.execute_reply": "2025-08-18T14:54:13.015667Z"
    },
    "papermill": {
     "duration": 133.82697,
     "end_time": "2025-08-18T14:54:13.018170",
     "exception": false,
     "start_time": "2025-08-18T14:51:59.191200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 14:53:51.462326: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1755528831.834629      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1755528831.936625      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# Install Hugging Face Transformers and related packages\n",
    "!pip install -q transformers accelerate einops decord\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import decord\n",
    "from decord import VideoReader, cpu\n",
    "from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification, VideoMAEConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a949eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:54:13.122986Z",
     "iopub.status.busy": "2025-08-18T14:54:13.122204Z",
     "iopub.status.idle": "2025-08-18T14:54:13.126394Z",
     "shell.execute_reply": "2025-08-18T14:54:13.125849Z"
    },
    "papermill": {
     "duration": 0.057063,
     "end_time": "2025-08-18T14:54:13.127541",
     "exception": false,
     "start_time": "2025-08-18T14:54:13.070478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57571c2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:54:13.230185Z",
     "iopub.status.busy": "2025-08-18T14:54:13.229950Z",
     "iopub.status.idle": "2025-08-18T14:54:13.233958Z",
     "shell.execute_reply": "2025-08-18T14:54:13.233453Z"
    },
    "papermill": {
     "duration": 0.056556,
     "end_time": "2025-08-18T14:54:13.234980",
     "exception": false,
     "start_time": "2025-08-18T14:54:13.178424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Class labels\n",
    "CLASSES = ['cover', 'defense', 'flick', 'hook', 'late_cut', 'lofted', 'pull', 'square_cut', 'straight', 'sweep']\n",
    "CLASS_TO_IDX = {cls: idx for idx, cls in enumerate(CLASSES)}\n",
    "\n",
    "# Dataset path\n",
    "DATASET_ROOT = \"/kaggle/working/split_data\"\n",
    "\n",
    "# Frame count (VideoMAE expects 16 for ViT-B base)\n",
    "NUM_FRAMES = 16\n",
    "BATCH_SIZE = 8  # Optimal for T4 2 setup\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2ca6ace",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:54:13.337580Z",
     "iopub.status.busy": "2025-08-18T14:54:13.337371Z",
     "iopub.status.idle": "2025-08-18T14:54:19.048670Z",
     "shell.execute_reply": "2025-08-18T14:54:19.047933Z"
    },
    "papermill": {
     "duration": 5.764617,
     "end_time": "2025-08-18T14:54:19.050501",
     "exception": false,
     "start_time": "2025-08-18T14:54:13.285884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31e1f372360f4f2aac691685a5f84b4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bb213934df8485cb40cf4031728507c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/377M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of VideoMAEForVideoClassification were not initialized from the model checkpoint at MCG-NJU/videomae-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c9ceda6d144bf5a191bda37ae7d203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/models/videomae/feature_extraction_videomae.py:28: FutureWarning: The class VideoMAEFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use VideoMAEImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model and set output classes\n",
    "model_name = \"MCG-NJU/videomae-base\"\n",
    "config = VideoMAEConfig.from_pretrained(model_name, num_labels=len(CLASSES))\n",
    "model = VideoMAEForVideoClassification.from_pretrained(model_name, config=config).to(DEVICE)\n",
    "\n",
    "# Multi-GPU support\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "# Load feature extractor\n",
    "feature_extractor = VideoMAEFeatureExtractor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f200cbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:54:19.158290Z",
     "iopub.status.busy": "2025-08-18T14:54:19.157795Z",
     "iopub.status.idle": "2025-08-18T14:54:19.166214Z",
     "shell.execute_reply": "2025-08-18T14:54:19.165677Z"
    },
    "papermill": {
     "duration": 0.06457,
     "end_time": "2025-08-18T14:54:19.167358",
     "exception": false,
     "start_time": "2025-08-18T14:54:19.102788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from decord import VideoReader, cpu\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class CricketVideoDataset(Dataset):\n",
    "    def __init__(self, video_paths, labels, feature_extractor, num_frames=16, mode=\"train\"):\n",
    "        self.video_paths = video_paths\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.num_frames = num_frames\n",
    "        self.mode = mode  # 'train', 'val', or 'test'\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "            self.augment = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),  # Preserve full frame info\n",
    "            transforms.RandomHorizontalFlip(p=0.5),  # Safe\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        else:\n",
    "            self.augment = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_path = self.video_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        vr = VideoReader(video_path, ctx=cpu(0))\n",
    "        total_frames = len(vr)\n",
    "\n",
    "        mid = total_frames // 2\n",
    "        half_span = self.num_frames // 2\n",
    "\n",
    "        # Select indices for 16 frames from center\n",
    "        if total_frames < self.num_frames:\n",
    "            indices = np.linspace(0, total_frames - 1, self.num_frames).astype(int)\n",
    "        else:\n",
    "            start = max(0, mid - half_span)\n",
    "            end = min(total_frames, mid + half_span + 1)\n",
    "            indices = np.linspace(start, end - 1, self.num_frames).astype(int)\n",
    "\n",
    "        # Decode and augment\n",
    "        video = vr.get_batch(indices).asnumpy()  # shape: (T, H, W, C)\n",
    "        augmented_frames = [self.augment(frame) for frame in video]  # [T, C, H, W]\n",
    "\n",
    "        # Convert to [T, H, W, C] for feature extractor\n",
    "        frames = [frame.permute(1, 2, 0).numpy() for frame in augmented_frames]\n",
    "\n",
    "        # Use Hugging Face feature extractor to normalize + convert to [C, T, H, W]\n",
    "        inputs = self.feature_extractor(frames, return_tensors=\"pt\")\n",
    "        pixel_values = inputs[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a398456",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:54:19.270285Z",
     "iopub.status.busy": "2025-08-18T14:54:19.270071Z",
     "iopub.status.idle": "2025-08-18T14:54:19.282226Z",
     "shell.execute_reply": "2025-08-18T14:54:19.281545Z"
    },
    "papermill": {
     "duration": 0.065047,
     "end_time": "2025-08-18T14:54:19.283413",
     "exception": false,
     "start_time": "2025-08-18T14:54:19.218366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_video_paths_and_labels(split_dir):\n",
    "    video_paths, labels = [], []\n",
    "    for cls in CLASSES:\n",
    "        cls_dir = os.path.join(split_dir, cls)\n",
    "        for file in os.listdir(cls_dir):\n",
    "            if file.endswith(\".mp4\"):\n",
    "                video_paths.append(os.path.join(cls_dir, file))\n",
    "                labels.append(CLASS_TO_IDX[cls])\n",
    "    return video_paths, labels\n",
    "\n",
    "# Train/Val/Test splits\n",
    "train_videos, train_labels = get_video_paths_and_labels(os.path.join(DATASET_ROOT, \"train\"))\n",
    "val_videos, val_labels = get_video_paths_and_labels(os.path.join(DATASET_ROOT, \"val\"))\n",
    "test_videos, test_labels = get_video_paths_and_labels(os.path.join(DATASET_ROOT, \"test\"))\n",
    "\n",
    "# Datasets\n",
    "train_dataset = CricketVideoDataset(train_videos, train_labels, feature_extractor, NUM_FRAMES, mode = \"train\")\n",
    "val_dataset = CricketVideoDataset(val_videos, val_labels, feature_extractor, NUM_FRAMES, mode=\"val\")\n",
    "test_dataset = CricketVideoDataset(test_videos, test_labels, feature_extractor, NUM_FRAMES, mode=\"test\")\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b9e8608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:54:19.386348Z",
     "iopub.status.busy": "2025-08-18T14:54:19.386164Z",
     "iopub.status.idle": "2025-08-18T14:54:19.417010Z",
     "shell.execute_reply": "2025-08-18T14:54:19.416552Z"
    },
    "papermill": {
     "duration": 0.0833,
     "end_time": "2025-08-18T14:54:19.418160",
     "exception": false,
     "start_time": "2025-08-18T14:54:19.334860",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# Training setup\n",
    "EPOCHS = 50\n",
    "val_accuracies = []\n",
    "learning_rate = 3e-5\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.02)\n",
    "loss_fn = CrossEntropyLoss()\n",
    "\n",
    "total_steps = len(train_loader) * EPOCHS\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=int(0.05 * total_steps), \n",
    "    num_training_steps=total_steps\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c2e449a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:54:19.522712Z",
     "iopub.status.busy": "2025-08-18T14:54:19.522116Z",
     "iopub.status.idle": "2025-08-18T14:54:19.528380Z",
     "shell.execute_reply": "2025-08-18T14:54:19.527770Z"
    },
    "papermill": {
     "duration": 0.059084,
     "end_time": "2025-08-18T14:54:19.529491",
     "exception": false,
     "start_time": "2025-08-18T14:54:19.470407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        #  Fix for multi-GPU: ensure scalar loss\n",
    "        if isinstance(loss, torch.Tensor) and loss.dim() > 0:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc, all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f45aee2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T14:54:19.630962Z",
     "iopub.status.busy": "2025-08-18T14:54:19.630775Z",
     "iopub.status.idle": "2025-08-18T23:43:57.143875Z",
     "shell.execute_reply": "2025-08-18T23:43:57.142954Z"
    },
    "papermill": {
     "duration": 31777.564944,
     "end_time": "2025-08-18T23:43:57.145141",
     "exception": false,
     "start_time": "2025-08-18T14:54:19.580197",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:41<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:51<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3233 | Val Accuracy: 0.0961\n",
      " Model saved.\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:41<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:51<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3202 | Val Accuracy: 0.0961\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:41<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3190 | Val Accuracy: 0.1014\n",
      " Model saved.\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:41<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3176 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:51<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3097 | Val Accuracy: 0.1032\n",
      " Model saved.\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:41<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3122 | Val Accuracy: 0.1068\n",
      " Model saved.\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:43<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:51<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3121 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3128 | Val Accuracy: 0.0996\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3107 | Val Accuracy: 0.1050\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:41<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3115 | Val Accuracy: 0.1050\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3104 | Val Accuracy: 0.0961\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:55<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3119 | Val Accuracy: 0.1032\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:43<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3094 | Val Accuracy: 0.0961\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:43<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3084 | Val Accuracy: 0.1032\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3098 | Val Accuracy: 0.0943\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3109 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3091 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:51<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3072 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3088 | Val Accuracy: 0.1050\n",
      "\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3069 | Val Accuracy: 0.0961\n",
      "\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3091 | Val Accuracy: 0.0961\n",
      "\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3072 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3079 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:41<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3070 | Val Accuracy: 0.1032\n",
      "\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:39<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3065 | Val Accuracy: 0.1050\n",
      "\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:39<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3068 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:39<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3058 | Val Accuracy: 0.1050\n",
      "\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:38<00:00,  1.75s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3050 | Val Accuracy: 0.0996\n",
      "\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:41<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3057 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:39<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:51<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3054 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:39<00:00,  1.76s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:51<00:00,  1.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3049 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3055 | Val Accuracy: 0.1050\n",
      "\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:43<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3053 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:43<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3049 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3041 | Val Accuracy: 0.1050\n",
      "\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3042 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3038 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:43<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:52<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3033 | Val Accuracy: 0.1014\n",
      "\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:43<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3034 | Val Accuracy: 0.1050\n",
      "\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3034 | Val Accuracy: 0.1050\n",
      "\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3029 | Val Accuracy: 0.1050\n",
      "\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3026 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3025 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:55<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3023 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:43<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3022 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3021 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3020 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:43<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:53<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3019 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:42<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:54<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3018 | Val Accuracy: 0.1068\n",
      "\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/330 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "Training: 100%|| 330/330 [09:43<00:00,  1.77s/it]\n",
      "Validating:   0%|          | 0/71 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Validating: 100%|| 71/71 [00:55<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 2.3018 | Val Accuracy: 0.1068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, scheduler)\n",
    "    val_acc, _, _ = evaluate(model, val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    val_accuracies.append(val_acc)\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_videomae_cricket.pth\")\n",
    "        print(\" Model saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6c49e05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T23:43:59.107404Z",
     "iopub.status.busy": "2025-08-18T23:43:59.107103Z",
     "iopub.status.idle": "2025-08-18T23:43:59.592917Z",
     "shell.execute_reply": "2025-08-18T23:43:59.592084Z"
    },
    "papermill": {
     "duration": 1.508615,
     "end_time": "2025-08-18T23:43:59.594551",
     "exception": false,
     "start_time": "2025-08-18T23:43:58.085936",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_test(model, dataloader, class_names):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Testing\"):\n",
    "            pixel_values = batch[\"pixel_values\"].to(DEVICE)\n",
    "            labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "            outputs = model(pixel_values=pixel_values)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\n Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names,\n",
    "                yticklabels=class_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "    return all_preds, all_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6500be0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T23:44:01.468775Z",
     "iopub.status.busy": "2025-08-18T23:44:01.468241Z",
     "iopub.status.idle": "2025-08-18T23:44:02.095269Z",
     "shell.execute_reply": "2025-08-18T23:44:02.094435Z"
    },
    "papermill": {
     "duration": 1.590148,
     "end_time": "2025-08-18T23:44:02.096446",
     "exception": false,
     "start_time": "2025-08-18T23:44:00.506298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAHWCAYAAABACtmGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABrrklEQVR4nO3dd3gU9fbH8c8mpENoCSTU0KQXQcCAUhRpCqJIVxDsgKCxIBaKXkH0CopdroBIEUEpXhREmqgIAlJUuiBKRwgJCSQh+f7+4Je9bLK7sxsSNgnv1/PwaCZz9pydPTu7Z2d2YjPGGAEAAAAAXPLzdQEAAAAAkN8xOAEAAACABQYnAAAAALDA4AQAAAAAFhicAAAAAMACgxMAAAAAWGBwAgAAAAALDE4AAAAAYIHBCQAAAAAsMDgBQA4cOHBANptN06dPty8bM2aMbDabR/E2m01jxozJ1ZratGmjNm3a5OptAgVNmzZtVK9ePV+XAaAQYnACUOh17dpVoaGhSkxMdLlOv379FBgYqH/++ecKVua933//XWPGjNGBAwd8XYpTX331lWw2m8qVK6eMjAxfl4M80KZNG9lsNqf/atWq5evyACDPFPF1AQCQ1/r166cvv/xSCxYsUP/+/bP9Pjk5WYsWLVLHjh1VunTpHOd5/vnn9cwzz1xOqZZ+//13jR07Vm3atFFMTIzD77755ps8ze2JWbNmKSYmRgcOHNDKlSvVrl07X5eEPFChQgWNHz8+2/LixYv7oBoAuDIYnAAUel27dlWxYsU0e/Zsp4PTokWLlJSUpH79+l1WniJFiqhIEd/tVgMDA32WW5KSkpK0aNEijR8/XtOmTdOsWbPy7eCUlJSksLAwX5eRL2VkZCg1NVXBwcEu1ylevLjuvvvuK1gVAPgep+oBKPRCQkJ05513asWKFTp+/Hi238+ePVvFihVT165dderUKT355JOqX7++ihYtqvDwcHXq1Elbt261zOPsO04pKSl6/PHHFRkZac/x999/Z4v9888/NXjwYNWsWVMhISEqXbq0evTo4XBK3vTp09WjRw9JUtu2be2nR61evVqS8+84HT9+XPfdd5/Kli2r4OBgNWzYUB9//LHDOpnf1/r3v/+tDz/8UNWqVVNQUJCaNm2qn3/+2fJ+Z1qwYIHOnTunHj16qHfv3vriiy90/vz5bOudP39eY8aM0TXXXKPg4GBFR0frzjvv1L59++zrZGRk6M0331T9+vUVHBysyMhIdezYURs3bnSo+dLvmGXK+v2xzMfl999/V9++fVWyZEndcMMNkqRt27bp3nvvVdWqVRUcHKyoqCgNGjTI6Smbhw4d0n333ady5copKChIVapU0SOPPKLU1FT98ccfstlsmjRpUra4H3/8UTabTXPmzHG7/aweq7S0NJUqVUoDBw7MFpuQkKDg4GA9+eST9mUpKSkaPXq0qlevrqCgIFWsWFFPP/20UlJSsm2voUOHatasWapbt66CgoK0dOlSt7V6InO779y5Uz179lR4eLhKly6t4cOHZ+uLCxcu6KWXXrL3XkxMjJ599tlstUrS119/rdatW6tYsWIKDw9X06ZNNXv27Gzr/f7772rbtq1CQ0NVvnx5vfrqq9nWeeutt1S3bl2FhoaqZMmSuu6665zeFgBIHHECcJXo16+fPv74Y3322WcaOnSoffmpU6e0bNky9enTRyEhIfrtt9+0cOFC9ejRQ1WqVNGxY8f0wQcfqHXr1vr9999Vrlw5r/Lef//9mjlzpvr27asWLVpo5cqVuvXWW7Ot9/PPP+vHH39U7969VaFCBR04cEDvvfee2rRpo99//12hoaFq1aqVhg0bpsmTJ+vZZ59V7dq1Jcn+36zOnTunNm3aaO/evRo6dKiqVKmiefPm6d5771V8fLyGDx/usP7s2bOVmJiohx56SDabTa+++qruvPNO/fHHHwoICLC8r7NmzVLbtm0VFRWl3r1765lnntGXX35pH/YkKT09XbfddptWrFih3r17a/jw4UpMTNTy5cv166+/qlq1apKk++67T9OnT1enTp10//3368KFC1q7dq1++uknXXfddR5v/0v16NFDNWrU0Lhx42SMkSQtX75cf/zxhwYOHKioqCj99ttv+vDDD/Xbb7/pp59+sg/Chw8fVrNmzRQfH68HH3xQtWrV0qFDhzR//nwlJyeratWqatmypWbNmqXHH38823YpVqyYbr/9dpe1efJYBQQE6I477tAXX3yhDz74wOEI48KFC5WSkqLevXtLujh4du3aVd9//70efPBB1a5dW9u3b9ekSZO0e/duLVy40CH/ypUr7c+NiIiIbKeBZpWenq6TJ09mWx4SEpLtSF7Pnj0VExOj8ePH66efftLkyZN1+vRpzZgxw77O/fffr48//lh33XWXnnjiCa1fv17jx4/Xjh07tGDBAvt606dP16BBg1S3bl2NHDlSJUqU0C+//KKlS5eqb9++9vVOnz6tjh076s4771TPnj01f/58jRgxQvXr11enTp0kSVOmTNGwYcN011132Ye5bdu2af369Q63BQB2BgCuAhcuXDDR0dEmNjbWYfn7779vJJlly5YZY4w5f/68SU9Pd1hn//79JigoyLz44osOyySZadOm2ZeNHj3aXLpb3bJli5FkBg8e7HB7ffv2NZLM6NGj7cuSk5Oz1bxu3TojycyYMcO+bN68eUaSWbVqVbb1W7dubVq3bm3/+Y033jCSzMyZM+3LUlNTTWxsrClatKhJSEhwuC+lS5c2p06dsq+7aNEiI8l8+eWX2XJldezYMVOkSBEzZcoU+7IWLVqY22+/3WG9qVOnGklm4sSJ2W4jIyPDGGPMypUrjSQzbNgwl+s42/6Zsm7bzMelT58+2dZ1tt3nzJljJJnvvvvOvqx///7Gz8/P/Pzzzy5r+uCDD4wks2PHDvvvUlNTTUREhBkwYEC2uEt5+lgtW7bM6WPSuXNnU7VqVfvPn3zyifHz8zNr1651WC+z33/44Qf7MknGz8/P/Pbbb25rzNS6dWsjyem/hx56yL5e5nbv2rWrQ/zgwYONJLN161ZjzP+eJ/fff7/Dek8++aSRZFauXGmMMSY+Pt4UK1bMNG/e3Jw7d85h3czH4NL6Ln3epKSkmKioKNO9e3f7sttvv93UrVvXo/sMAMYYw6l6AK4K/v7+6t27t9atW+dw+tvs2bNVtmxZ3XzzzZKkoKAg+fld3DWmp6frn3/+UdGiRVWzZk1t3rzZq5xfffWVJGnYsGEOyx977LFs64aEhNj/Py0tTf/884+qV6+uEiVKeJ330vxRUVHq06ePfVlAQICGDRums2fPas2aNQ7r9+rVSyVLlrT/fOONN0qS/vjjD8tcn376qfz8/NS9e3f7sj59+ujrr7/W6dOn7cs+//xzRURE6NFHH812G5lHdz7//HPZbDaNHj3a5To58fDDD2dbdul2P3/+vE6ePKnrr79ekuzbPSMjQwsXLlSXLl2cHu3KrKlnz54KDg7WrFmz7L9btmyZTp48afl9IE8fq5tuukkRERGaO3eufb3Tp09r+fLl6tWrl33ZvHnzVLt2bdWqVUsnT560/7vpppskSatWrXLI37p1a9WpU8dtjZeKiYnR8uXLs/1z1ttDhgxx+Dnzsc98fmT+Ny4uzmG9J554QpK0ZMkSSRePDiYmJuqZZ57J9v2rrH1RtGhRh20eGBioZs2aOfRyiRIl9Pfff3t1OiqAqxuDE4CrRubFHzK/w/D3339r7dq16t27t/z9/SVdfJM8adIk1ahRQ0FBQYqIiFBkZKS2bdumM2fOeJXvzz//lJ+fn/30s0w1a9bMtu65c+c0atQoVaxY0SFvfHy813kvzV+jRg37IJgp89S+P//802F5pUqVHH7OHKIuHXxcmTlzppo1a6Z//vlHe/fu1d69e3XttdcqNTVV8+bNs6+3b98+1axZ0+1FNPbt26dy5cqpVKlSlnm9UaVKlWzLTp06peHDh6ts2bIKCQlRZGSkfb3M7X7ixAklJCRY/m2gEiVKqEuXLg7fkZk1a5bKly9vH1hc8fSxKlKkiLp3765FixbZv//zxRdfKC0tzWFw2rNnj3777TdFRkY6/LvmmmskKdt3/ZxtG3fCwsLUrl27bP+cXY68Ro0aDj9Xq1ZNfn5+9g8wMp8n1atXd1gvKipKJUqUsN/3zO/AefI3mipUqJBtmCpZsqRDL48YMUJFixZVs2bNVKNGDQ0ZMkQ//PCD9Z0HcNXiO04ArhpNmjRRrVq1NGfOHD377LOaM2eOjDEOV9MbN26cXnjhBQ0aNEgvvfSSSpUqJT8/Pz322GN5+neJHn30UU2bNk2PPfaYYmNjVbx4cdlsNvXu3fuK/T2kzOExK/P/3wdyZc+ePfZP7bO+SZYuDg8PPvjg5Rd4CVdHntLT013GXHp0KVPPnj31448/6qmnnlKjRo1UtGhRZWRkqGPHjjna7v3799e8efP0448/qn79+lq8eLEGDx6cbSC6HL1799YHH3ygr7/+Wt26ddNnn32mWrVqqWHDhvZ1MjIyVL9+fU2cONHpbVSsWNHhZ2fbJq+4euwu52hiVp70cu3atbVr1y7997//1dKlS/X555/r3Xff1ahRozR27NhcqwVA4cHgBOCq0q9fP73wwgvatm2bZs+erRo1aqhp06b238+fP19t27bVRx995BAXHx+viIgIr3JVrlxZGRkZ9qMsmXbt2pVt3fnz52vAgAF6/fXX7cvOnz+v+Ph4h/W8eXNZuXJlbdu2TRkZGQ5v3Hfu3Gn/fW6YNWuWAgIC9Mknn2R7w/r9999r8uTJOnjwoCpVqqRq1app/fr1SktLc3nBiWrVqmnZsmU6deqUy6NOmUfDsm6frEfR3Dl9+rRWrFihsWPHatSoUfble/bscVgvMjJS4eHh+vXXXy1vs2PHjoqMjNSsWbPUvHlzJScn65577rGM8+axatWqlaKjozV37lzdcMMNWrlypZ577jmH26tWrZq2bt2qm2++OVcHkpzYs2ePwxGtvXv3KiMjw34BisznyZ49exwudHLs2DHFx8fb73vmkdtff/0129GpnAoLC1OvXr3Uq1cvpaam6s4779TLL7+skSNHur0cO4CrE6fqAbiqZB5dGjVqlLZs2ZLtbzf5+/tnO8Iyb948HTp0yOtcmVfvmjx5ssPyN954I9u6zvK+9dZb2Y6gZF6xLOvA4Eznzp119OhRh+/DXLhwQW+99ZaKFi2q1q1be3I3LM2aNUs33nijevXqpbvuusvh31NPPSVJ9ktxd+/eXSdPntTbb7+d7XYy73/37t1ljHH6qX/mOuHh4YqIiNB3333n8Pt3333X47ozh7ys2z3r4+Pn56du3brpyy+/tF8O3VlN0sVT6fr06aPPPvtM06dPV/369dWgQQPLWrx5rPz8/HTXXXfpyy+/1CeffKILFy44nKYnXTySdujQIU2ZMiVbrnPnzikpKcmyptzyzjvvOPz81ltvSfrf86Nz586Ssm/3zKNlmVehbN++vYoVK6bx48dnu5y51VFRZ7Jecj4wMFB16tSRMUZpaWle3x6Awo8jTgCuKlWqVFGLFi20aNEiSco2ON1222168cUXNXDgQLVo0ULbt2/XrFmzVLVqVa9zNWrUSH369NG7776rM2fOqEWLFlqxYoX27t2bbd3bbrtNn3zyiYoXL646depo3bp1+vbbb1W6dOlst+nv768JEybozJkzCgoK0k033aQyZcpku80HH3xQH3zwge69915t2rRJMTExmj9/vn744Qe98cYbKlasmNf3Kav169fbL6HtTPny5dW4cWPNmjVLI0aMUP/+/TVjxgzFxcVpw4YNuvHGG5WUlKRvv/1WgwcP1u233662bdvqnnvu0eTJk7Vnzx77aXNr165V27Zt7bnuv/9+vfLKK7r//vt13XXX6bvvvtPu3bs9rj08PFytWrXSq6++qrS0NJUvX17ffPON9u/fn23dcePG6ZtvvlHr1q3tl/c+cuSI5s2bp++//14lSpSwr9u/f39NnjxZq1at0oQJEzyqxdvHqlevXnrrrbc0evRo1a9fP9sl6e+55x599tlnevjhh7Vq1Sq1bNlS6enp2rlzpz777DMtW7Ysx5d1ly5+/2vmzJlOf5f1Qhj79+9X165d1bFjR61bt85+ef7MUwsbNmyoAQMG6MMPP1R8fLxat26tDRs26OOPP1a3bt3Utm1bSRcfr0mTJun+++9X06ZN7X+Ta+vWrUpOTs7298mstG/fXlFRUWrZsqXKli2rHTt26O2339att96aK88NAIWQby7mBwC+88477xhJplmzZtl+d/78efPEE0+Y6OhoExISYlq2bGnWrVuX7VLfnlyO3Bhjzp07Z4YNG2ZKly5twsLCTJcuXcxff/2V7ZLZp0+fNgMHDjQRERGmaNGipkOHDmbnzp2mcuXK2S5lPWXKFFO1alXj7+/vcGnyrDUac/Ey4Zm3GxgYaOrXr5/tEt6Z9+W1117Ltj2y1pnVo48+aiSZffv2uVxnzJgxDpefTk5ONs8995ypUqWKCQgIMFFRUeauu+5yuI0LFy6Y1157zdSqVcsEBgaayMhI06lTJ7Np0yb7OsnJyea+++4zxYsXN8WKFTM9e/Y0x48fd3k58hMnTmSr7e+//zZ33HGHKVGihClevLjp0aOHOXz4sNP7/eeff5r+/fubyMhIExQUZKpWrWqGDBliUlJSst1u3bp1jZ+fn/n7779dbpesPHmsMmVkZJiKFSsaSeZf//qX03VSU1PNhAkTTN26dU1QUJApWbKkadKkiRk7dqw5c+aMfT1JZsiQIR7X6e5y5Jf2f+Z2//33381dd91lihUrZkqWLGmGDh2a7XLiaWlpZuzYsfaeqFixohk5cqQ5f/58tvyLFy82LVq0MCEhISY8PNw0a9bMzJkzx6E+Z5cZHzBggKlcubL95w8++MC0atXKlC5d2gQFBZlq1aqZp556ymHbAMClbMbk4Pg2AABw6dprr1WpUqW0YsUKX5fiM2PGjNHYsWN14sQJr78fCAD5Ed9xAgAgF23cuFFbtmxR//79fV0KACAX8R0nAABywa+//qpNmzbp9ddfV3R0dLYLNgAACjaOOAEAkAvmz5+vgQMHKi0tTXPmzOFy1gBQyPh0cPruu+/UpUsXlStXTjabTQsXLrSMWb16tRo3bqygoCBVr15d06dPz/M6AQCwMmbMGGVkZGjHjh25dqn3gmzMmDEyxvD9JgCFhk8Hp6SkJDVs2DDb33hwZf/+/br11lvVtm1bbdmyRY899pjuv/9+LVu2LI8rBQAAAHA1yzdX1bPZbFqwYIG6devmcp0RI0ZoyZIlDn+9vXfv3oqPj9fSpUuvQJUAAAAArkYF6uIQ69atU7t27RyWdejQQY899pjLmJSUFKWkpNh/zsjI0KlTp1S6dGnZbLa8KhUAAABAPmeMUWJiosqVKyc/P/cn4xWoweno0aMqW7asw7KyZcsqISFB586dU0hISLaY8ePHa+zYsVeqRAAAAAAFzF9//aUKFSq4XadADU45MXLkSMXFxdl/PnPmjCpVqqT9+/erWLFiPqzsorS0NK1atUpt27ZVQEBAvo2j1vyXk1oLT05qLTw5qbXw5CxItfoiJ7UWnpwFqda8kJiYqCpVqng0FxSowSkqKkrHjh1zWHbs2DGFh4c7PdokSUFBQQoKCsq2vFSpUgoPD8+TOr2Rlpam0NBQlS5d2uuGu5Jx1Jr/clJr4clJrYUnJ7UWnpwFqVZf5KTWwpOzINWaFzLze/IVngL1d5xiY2O1YsUKh2XLly9XbGysjyoCAAAAcDXw6eB09uxZbdmyRVu2bJF08XLjW7Zs0cGDByVdPM2uf//+9vUffvhh/fHHH3r66ae1c+dOvfvuu/rss8/0+OOP+6J8AAAAAFcJnw5OGzdu1LXXXqtrr71WkhQXF6drr71Wo0aNkiQdOXLEPkRJUpUqVbRkyRItX75cDRs21Ouvv67//Oc/6tChg0/qBwAAAHB18Ol3nNq0aSN3f0Zq+vTpTmN++eWXPKwKAAAAABwVqO84AQAAAIAvMDgBAAAAgAUGJwAAAACwwOAEAAAAABYYnAAAAADAAoMTAAAAAFhgcAIAAAAACwxOAAAAAGCBwQkAAAAALDA4AQAAAIAFBicAAAAAsMDgBAAAAAAWGJwAAAAAwAKDEwAAAABYYHACAAAAAAsMTgAAAABggcEJAAAAACwwOAEAAACABQYnAAAAALDA4AQAAAAAFhicAAAAAMACgxMAAAAAWGBwAgAAAAALDE4AAAAAYIHBCQAAAAAsMDgBAAAAgAUGJwAAAACwwOAEAAAAABYYnAAAAADAAoMTAAAAAFhgcAIAAAAACwxOAAAAAGCBwQkAAAAALDA4AQAAAIAFBicAAAAAsMDgBAAAAAAWGJwAAAAAwAKDEwAAAABYYHACAAAAAAsMTgAAAABggcEJAAAAACwwOAEAAACABQYnAAAAALDA4AQAAAAAFhicAAAAAMACgxMAAAAAWGBwAgAAAAALDE4AAAAAYIHBCQAAAAAsMDgBAAAAgAUGJwAAAACwwOAEAAAAABYYnAAAAADAAoMTAAAAAFhgcAIAAAAACwxOAAAAAGCBwQkAAAAALDA4AQAAAIAFBicAAAAAsMDgBAAAAAAWGJwAAAAAwAKDEwAAAABYYHACAAAAAAsMTgAAAABggcEJAAAAACwwOAEAAACABZ8PTu+8845iYmIUHBys5s2ba8OGDW7Xf+ONN1SzZk2FhISoYsWKevzxx3X+/PkrVC0AAACAq5FPB6e5c+cqLi5Oo0eP1ubNm9WwYUN16NBBx48fd7r+7Nmz9cwzz2j06NHasWOHPvroI82dO1fPPvvsFa4cAAAAwNXEp4PTxIkT9cADD2jgwIGqU6eO3n//fYWGhmrq1KlO1//xxx/VsmVL9e3bVzExMWrfvr369OljeZQKAAAAAC5HEV8lTk1N1aZNmzRy5Ej7Mj8/P7Vr107r1q1zGtOiRQvNnDlTGzZsULNmzfTHH3/oq6++0j333OMyT0pKilJSUuw/JyQkSJLS0tKUlpaWS/cm5zJr8LaWKx3ni5wFqVZf5KTWwpOTWgtPTmotPDkLUq2+yEmthSdnQao1L3hTg80YY/KwFpcOHz6s8uXL68cff1RsbKx9+dNPP601a9Zo/fr1TuMmT56sJ598UsYYXbhwQQ8//LDee+89l3nGjBmjsWPHZls+e/ZshYaGXv4dAQAAAFAgJScnq2/fvjpz5ozCw8PdruuzI045sXr1ao0bN07vvvuumjdvrr1792r48OF66aWX9MILLziNGTlypOLi4uw/JyQkqGLFimrfvr3lxrkS0tLStHz5ct1yyy0KCAjIt3HUmv9yUmvhyUmthScntRaenAWpVl/kpNbCk7Mg1ZoXMs9G84TPBqeIiAj5+/vr2LFjDsuPHTumqKgopzEvvPCC7rnnHt1///2SpPr16yspKUkPPvignnvuOfn5Zf/KVlBQkIKCgrItDwgI8PkDdamc1nOl43yRsyDV6ouc1Fp4clJr4clJrYUnZ0Gq1Rc5qbXw5CxIteYmb/L77OIQgYGBatKkiVasWGFflpGRoRUrVjicunep5OTkbMORv7+/JMlHZxwCAAAAuAr49FS9uLg4DRgwQNddd52aNWumN954Q0lJSRo4cKAkqX///ipfvrzGjx8vSerSpYsmTpyoa6+91n6q3gsvvKAuXbrYBygAAAAAyG0+HZx69eqlEydOaNSoUTp69KgaNWqkpUuXqmzZspKkgwcPOhxhev7552Wz2fT888/r0KFDioyMVJcuXfTyyy/76i4AAAAAuAr4/OIQQ4cO1dChQ53+bvXq1Q4/FylSRKNHj9bo0aOvQGUAAAAAcJFP/wAuAAAAABQEDE4AAAAAYIHBCQAAAAAsMDgBAAAAgAUGJwAAAACwwOAEAAAAABYYnAAAAADAAoMTAAAAAFhgcAIAAAAACwxOAAAAAGCBwQkAAAAALDA4AQAAAIAFBicAAAAAsMDgBAAAAAAWGJwAAAAAwAKDEwAAAABYYHACAAAAAAsMTgAAAABggcEJAAAAACwwOAEAAACABQYnAAAAALDA4AQAAAAAFhicAAAAAMACgxMAAAAAWGBwAgAAAAALDE4AAAAAYIHBCQAAAAAsMDgBAAAAgAUGJwAAAACwwOAEAAAAABYYnAAAAADAAoMTAAAAAFhgcAIAAAAACwxOAAAAAGCBwQkAAAAALDA4AQAAAIAFBicAAAAAsMDgBAAAAAAWGJwAAAAAwAKDEwAAAABYYHACAAAAAAteD05//PFHXtQBAAAAAPmW14NT9erV1bZtW82cOVPnz5/Pi5oAAAAAIF/xenDavHmzGjRooLi4OEVFRemhhx7Shg0b8qI2AAAAAMgXvB6cGjVqpDfffFOHDx/W1KlTdeTIEd1www2qV6+eJk6cqBMnTuRFnQAAAADgMzm+OESRIkV05513at68eZowYYL27t2rJ598UhUrVlT//v115MiR3KwTAAAAAHwmx4PTxo0bNXjwYEVHR2vixIl68skntW/fPi1fvlyHDx/W7bffnpt1AgAAAIDPFPE2YOLEiZo2bZp27dqlzp07a8aMGercubP8/C7OYFWqVNH06dMVExOT27UCAAAAgE94PTi99957GjRokO69915FR0c7XadMmTL66KOPLrs4AAAAAMgPvB6c9uzZY7lOYGCgBgwYkKOCAAAAACC/8fo7TtOmTdO8efOyLZ83b54+/vjjXCkKAAAAAPITrwen8ePHKyIiItvyMmXKaNy4cblSFAAAAADkJ14PTgcPHlSVKlWyLa9cubIOHjyYK0UBAAAAQH7i9eBUpkwZbdu2LdvyrVu3qnTp0rlSFAAAAADkJ14PTn369NGwYcO0atUqpaenKz09XStXrtTw4cPVu3fvvKgRAAAAAHzK66vqvfTSSzpw4IBuvvlmFSlyMTwjI0P9+/fnO04AAAAACiWvB6fAwEDNnTtXL730krZu3aqQkBDVr19flStXzov6AAAAAMDnvB6cMl1zzTW65pprcrMWAAAAAMiXcjQ4/f3331q8eLEOHjyo1NRUh99NnDgxVwoDAAAAgPzC68FpxYoV6tq1q6pWraqdO3eqXr16OnDggIwxaty4cV7UCAAAAAA+5fVV9UaOHKknn3xS27dvV3BwsD7//HP99ddfat26tXr06JEXNQIAAACAT3k9OO3YsUP9+/eXJBUpUkTnzp1T0aJF9eKLL2rChAm5XiAAAAAA+JrXg1NYWJj9e03R0dHat2+f/XcnT57MvcoAAAAAIJ/w+jtO119/vb7//nvVrl1bnTt31hNPPKHt27friy++0PXXX58XNQIAAACAT3k9OE2cOFFnz56VJI0dO1Znz57V3LlzVaNGDa6oBwAAAKBQ8mpwSk9P199//60GDRpIunja3vvvv58nhQEAAABAfuHVd5z8/f3Vvn17nT59OtcKeOeddxQTE6Pg4GA1b95cGzZscLt+fHy8hgwZoujoaAUFBemaa67RV199lWv1AAAAAEBWXl8col69evrjjz9yJfncuXMVFxen0aNHa/PmzWrYsKE6dOig48ePO10/NTVVt9xyiw4cOKD58+dr165dmjJlisqXL58r9QAAAACAM14PTv/617/05JNP6r///a+OHDmihIQEh3/emDhxoh544AENHDhQderU0fvvv6/Q0FBNnTrV6fpTp07VqVOntHDhQrVs2VIxMTFq3bq1GjZs6O3dAAAAAACPeX1xiM6dO0uSunbtKpvNZl9ujJHNZlN6erpHt5OamqpNmzZp5MiR9mV+fn5q166d1q1b5zRm8eLFio2N1ZAhQ7Ro0SJFRkaqb9++GjFihPz9/Z3GpKSkKCUlxf5z5nCXlpamtLQ0j2rNS5k1eFvLlY7zRc6CVKsvclJr4clJrYUnJ7UWnpwFqVZf5KTWwpOzINWaF7ypwWaMMd7c+Jo1a9z+vnXr1h7dzuHDh1W+fHn9+OOPio2NtS9/+umntWbNGq1fvz5bTK1atXTgwAH169dPgwcP1t69ezV48GANGzZMo0ePdppnzJgxGjt2bLbls2fPVmhoqEe1AgAAACh8kpOT1bdvX505c0bh4eFu1/X6iJOng1FeyMjIUJkyZfThhx/K399fTZo00aFDh/Taa6+5HJxGjhypuLg4+88JCQmqWLGi2rdvb7lxroS0tDQtX75ct9xyiwICAvJtHLXmv5zUWnhyUmvhyUmthSdnQarVFzmptfDkLEi15gVvvmrk9eD03Xffuf19q1atPLqdiIgI+fv769ixYw7Ljx07pqioKKcx0dHRCggIcDgtr3bt2jp69KhSU1MVGBiYLSYoKEhBQUHZlgcEBPj8gbpUTuu50nG+yFmQavVFTmotPDmptfDkpNbCk7Mg1eqLnNRaeHIWpFpzkzf5vR6c2rRpk23Zpd918vQ7ToGBgWrSpIlWrFihbt26Sbp4RGnFihUaOnSo05iWLVtq9uzZysjIkJ/fxeta7N69W9HR0U6HJgAAAADIDV5fVe/06dMO/44fP66lS5eqadOm+uabb7y6rbi4OE2ZMkUff/yxduzYoUceeURJSUkaOHCgJKl///4OF4945JFHdOrUKQ0fPly7d+/WkiVLNG7cOA0ZMsTbuwEAAAAAHvP6iFPx4sWzLbvlllsUGBiouLg4bdq0yePb6tWrl06cOKFRo0bp6NGjatSokZYuXaqyZctKkg4ePGg/siRJFStW1LJly/T444+rQYMGKl++vIYPH64RI0Z4ezcAAAAAwGNeD06ulC1bVrt27fI6bujQoS5PzVu9enW2ZbGxsfrpp5+8zgMAAAAAOeX14LRt2zaHn40xOnLkiF555RU1atQot+oCAAAAgHzD68GpUaNGstlsyvrnn66//npNnTo11woDAAAAgPzC68Fp//79Dj/7+fkpMjJSwcHBuVYUAAAAAOQnXg9OlStXzos6AAAAACDf8vpy5MOGDdPkyZOzLX/77bf12GOP5UZNAAAAAJCveD04ff7552rZsmW25S1atND8+fNzpSgAAAAAyE+8Hpz++ecfp3/LKTw8XCdPnsyVogAAAAAgP/F6cKpevbqWLl2abfnXX3+tqlWr5kpRAAAAAJCfeH1xiLi4OA0dOlQnTpzQTTfdJElasWKFXn/9db3xxhu5XR8AAAAA+JzXg9OgQYOUkpKil19+WS+99JIkKSYmRu+995769++f6wUCAAAAgK95PThJ0iOPPKJHHnlEJ06cUEhIiIoWLZrbdQEAAABAvpGjP4B74cIF1ahRQ5GRkfble/bsUUBAgGJiYnKzPgAAAADwOa8vDnHvvffqxx9/zLZ8/fr1uvfee3OjJgAAAADIV7wenH755Renf8fp+uuv15YtW3KjJgAAAADIV7wenGw2mxITE7MtP3PmjNLT03OlKAAAAADIT7wenFq1aqXx48c7DEnp6ekaP368brjhhlwtDgAAAADyA68vDjFhwgS1atVKNWvW1I033ihJWrt2rRISErRy5cpcLxAAAAAAfM3rI0516tTRtm3b1LNnTx0/flyJiYnq37+/du7cqXr16uVFjQAAAADgUzn6O07lypXTuHHjHJbFx8fr7bff1tChQ3OlMAAAAADIL7w+4pTVihUr1LdvX0VHR2v06NG5URMAAAAA5Cs5Gpz++usvvfjii6pSpYrat28vSVqwYIGOHj2aq8UBAAAAQH7g8eCUlpamefPmqUOHDqpZs6a2bNmi1157TX5+fnr++efVsWNHBQQE5GWtAAAAAOATHn/HqXz58qpVq5buvvtuffrppypZsqQkqU+fPnlWHAAAAADkBx4fcbpw4YJsNptsNpv8/f3zsiYAAAAAyFc8HpwOHz6sBx98UHPmzFFUVJS6d++uBQsWyGaz5WV9AAAAAOBzHg9OwcHB6tevn1auXKnt27erdu3aGjZsmC5cuKCXX35Zy5cvV3p6el7WCgAAAAA+kaOr6lWrVk3/+te/9Oeff2rJkiVKSUnRbbfdprJly+Z2fQAAAADgczn6A7iZ/Pz81KlTJ3Xq1EknTpzQJ598klt1AQAAAEC+cdl/ADdTZGSk4uLicuvmAAAAACDfyLXBCQAAAAAKKwYnAAAAALDA4AQAAAAAFhicAAAAAMCC11fVS09P1/Tp07VixQodP35cGRkZDr9fuXJlrhUHAAAAAPmB14PT8OHDNX36dN16662qV6+ebDZbXtQFAAAAAPmG14PTp59+qs8++0ydO3fOi3oAAAAAIN/x+jtOgYGBql69el7UAgAAAAD5kteD0xNPPKE333xTxpi8qAcAAAAA8h2vT9X7/vvvtWrVKn399deqW7euAgICHH7/xRdf5FpxAAAAAJAfeD04lShRQnfccUde1AIAAAAA+ZLXg9O0adPyog4AAAAAyLe8HpwynThxQrt27ZIk1axZU5GRkblWFAAAAADkJ15fHCIpKUmDBg1SdHS0WrVqpVatWqlcuXK67777lJycnBc1AgAAAIBPeT04xcXFac2aNfryyy8VHx+v+Ph4LVq0SGvWrNETTzyRFzUCAAAAgE95fare559/rvnz56tNmzb2ZZ07d1ZISIh69uyp9957LzfrAwAAAACf8/qIU3JyssqWLZtteZkyZThVDwAAAECh5PXgFBsbq9GjR+v8+fP2ZefOndPYsWMVGxubq8UBAAAAQH7g9al6b775pjp06KAKFSqoYcOGkqStW7cqODhYy5Yty/UCAQAAAMDXvB6c6tWrpz179mjWrFnauXOnJKlPnz7q16+fQkJCcr1AAAAAAPC1HP0dp9DQUD3wwAO5XQsAAAAA5EseDU6LFy9Wp06dFBAQoMWLF7tdt2vXrrlSGAAAAADkFx4NTt26ddPRo0dVpkwZdevWzeV6NptN6enpuVUbAAAAAOQLHg1OGRkZTv8fAAAAAK4GXl+OfMaMGUpJScm2PDU1VTNmzMiVogAAAAAgP/F6cBo4cKDOnDmTbXliYqIGDhyYK0UBAAAAQH7i9eBkjJHNZsu2/O+//1bx4sVzpSgAAAAAyE88vhz5tddeK5vNJpvNpptvvllFivwvND09Xfv371fHjh3zpEgAAAAA8CWPB6fMq+lt2bJFHTp0UNGiRe2/CwwMVExMjLp3757rBQIAAACAr3k8OI0ePVqSFBMTo169eik4ODjPigIAAACA/MTjwSnTgAED8qIOAAAAAMi3vB6c0tPTNWnSJH322Wc6ePCgUlNTHX5/6tSpXCsOAAAAAPIDr6+qN3bsWE2cOFG9evXSmTNnFBcXpzvvvFN+fn4aM2ZMHpQIAAAAAL7l9eA0a9YsTZkyRU888YSKFCmiPn366D//+Y9GjRqln376KS9qBAAAAACf8npwOnr0qOrXry9JKlq0qP2P4d52221asmRJ7lYHAAAAAPmA14NThQoVdOTIEUlStWrV9M0330iSfv75ZwUFBeVudQAAAACQD3g9ON1xxx1asWKFJOnRRx/VCy+8oBo1aqh///4aNGhQrhcIAAAAAL7m9eD0yiuv6Nlnn5Uk9erVS999950eeeQRzZ8/X6+88kqOinjnnXcUExOj4OBgNW/eXBs2bPAo7tNPP5XNZrP/cV4AAAAAyAteX448q9jYWMXGxuY4fu7cuYqLi9P777+v5s2b64033lCHDh20a9culSlTxmXcgQMH9OSTT+rGG2/McW4AAAAA8IRHg9PixYs9vsGuXbt6VcDEiRP1wAMPaODAgZKk999/X0uWLNHUqVP1zDPPOI1JT09Xv379NHbsWK1du1bx8fFe5QQAAAAAb3g0OGU9Fc5ms8kYk22ZdHGo8VRqaqo2bdqkkSNH2pf5+fmpXbt2Wrduncu4F198UWXKlNF9992ntWvXus2RkpKilJQU+88JCQmSpLS0NKWlpXlca17JrMHbWq50nC9yFqRafZGTWgtPTmotPDmptfDkLEi1+iIntRaenAWp1rzgTQ02k3UCsvDtt99qxIgRGjdunP0UvXXr1un555/XuHHjdMstt3h8W4cPH1b58uX1448/Opzu9/TTT2vNmjVav359tpjvv/9evXv31pYtWxQREaF7771X8fHxWrhwodMcY8aM0dixY7Mtnz17tkJDQz2uFQAAAEDhkpycrL59++rMmTMKDw93u67X33F67LHH9P777+uGG26wL+vQoYNCQ0P14IMPaseOHd5X7KHExETdc889mjJliiIiIjyKGTlypOLi4uw/JyQkqGLFimrfvr3lxrkS0tLStHz5ct1yyy0KCAjIt3HUmv9yUmvhyUmthScntRaenAWpVl/kpNbCk7Mg1ZoXMs9G84TXg9O+fftUokSJbMuLFy+uAwcOeHVbERER8vf317FjxxyWHzt2TFFRUU5zHzhwQF26dLEvy8jIkCQVKVJEu3btUrVq1RxigoKCnP59qYCAAJ8/UJfKaT1XOs4XOQtSrb7ISa2FJye1Fp6c1Fp4chakWn2Rk1oLT86CVGtu8ia/15cjb9q0qeLi4hyGnWPHjumpp55Ss2bNvLqtwMBANWnSxP53oaSLg9CKFSucXqmvVq1a2r59u7Zs2WL/17VrV7Vt21ZbtmxRxYoVvb07AAAAAGDJ6yNOU6dO1R133KFKlSrZB5W//vpLNWrUcPk9I3fi4uI0YMAAXXfddWrWrJneeOMNJSUl2a+y179/f5UvX17jx49XcHCw6tWr5xCfefQr63IAAAAAyC1eD07Vq1fXtm3btHz5cu3cuVOSVLt2bbVr185+ZT1v9OrVSydOnNCoUaN09OhRNWrUSEuXLlXZsmUlSQcPHpSfn9cHxgAAAAAg1+ToD+DabDa1b99e7du3z5Uihg4dqqFDhzr93erVq93GTp8+PVdqAAAAAABXPBqcJk+erAcffFDBwcGaPHmy23WHDRuWK4UBAAAAQH7h0eA0adIk9evXT8HBwZo0aZLL9Ww2G4MTAAAAgELHo8Fp//79Tv8fAAAAAK4GXHUBAAAAACx4dMQpLi7O4xucOHFijosBAAAAgPzIo8Hpl19+8ejGcnI5cgAAAADI7zwanFatWpXXdQAAAABAvsV3nAAAAADAQo7+AO7GjRv12Wef6eDBg0pNTXX43RdffJErhQEAAABAfuH1EadPP/1ULVq00I4dO7RgwQKlpaXpt99+08qVK1W8ePG8qBEAAAAAfMrrwWncuHGaNGmSvvzySwUGBurNN9/Uzp071bNnT1WqVCkvagQAAAAAn/J6cNq3b59uvfVWSVJgYKCSkpJks9n0+OOP68MPP8z1AgEAAADA17wenEqWLKnExERJUvny5fXrr79KkuLj45WcnJy71QEAAABAPuD1xSFatWql5cuXq379+urRo4eGDx+ulStXavny5br55pvzokYAAAAA8CmPB6dff/1V9erV09tvv63z589Lkp577jkFBAToxx9/VPfu3fX888/nWaEAAAAA4CseD04NGjRQ06ZNdf/996t3796SJD8/Pz3zzDN5VhwAAAAA5Acef8dpzZo1qlu3rp544glFR0drwIABWrt2bV7WBgAAAAD5gseD04033qipU6fqyJEjeuutt3TgwAG1bt1a11xzjSZMmKCjR4/mZZ0AAAAA4DNeX1UvLCxMAwcO1Jo1a7R792716NFD77zzjipVqqSuXbvmRY0AAAAA4FNeD06Xql69up599lk9//zzKlasmJYsWZJbdQEAAABAvuH15cgzfffdd5o6dao+//xz+fn5qWfPnrrvvvtyszYAAAAAyBe8GpwOHz6s6dOna/r06dq7d69atGihyZMnq2fPngoLC8urGgEAAADApzwenDp16qRvv/1WERER6t+/vwYNGqSaNWvmZW0AAAAAkC94PDgFBARo/vz5uu222+Tv75+XNQEAAABAvuLx4LR48eK8rAMAAAAA8q3LuqoeAAAAAFwNGJwAAAAAwAKDEwAAAABYYHACAAAAAAsMTgAAAABggcEJAAAAACwwOAEAAACABQYnAAAAALDA4AQAAAAAFhicAAAAAMACgxMAAAAAWGBwAgAAAAALDE4AAAAAYIHBCQAAAAAsMDgBAAAAgAUGJwAAAACwwOAEAAAAABYYnAAAAADAAoMTAAAAAFhgcAIAAAAACwxOAAAAAGCBwQkAAAAALDA4AQAAAIAFBicAAAAAsMDgBAAAAAAWGJwAAAAAwAKDEwAAAABYYHACAAAAAAsMTgAAAABggcEJAAAAACwwOAEAAACABQYnAAAAALDA4AQAAAAAFhicAAAAAMACgxMAAAAAWGBwAgAAAAALDE4AAAAAYIHBCQAAAAAsMDgBAAAAgAUGJwAAAACwwOAEAAAAABbyxeD0zjvvKCYmRsHBwWrevLk2bNjgct0pU6boxhtvVMmSJVWyZEm1a9fO7foAAAAAcLl8PjjNnTtXcXFxGj16tDZv3qyGDRuqQ4cOOn78uNP1V69erT59+mjVqlVat26dKlasqPbt2+vQoUNXuHIAAAAAVwufD04TJ07UAw88oIEDB6pOnTp6//33FRoaqqlTpzpdf9asWRo8eLAaNWqkWrVq6T//+Y8yMjK0YsWKK1w5AAAAgKtFEV8mT01N1aZNmzRy5Ej7Mj8/P7Vr107r1q3z6DaSk5OVlpamUqVKOf19SkqKUlJS7D8nJCRIktLS0pSWlnYZ1eeOzBq8reVKx/kiZ0Gq1Rc5qbXw5KTWwpOTWgtPzoJUqy9yUmvhyVmQas0L3tRgM8aYPKzFrcOHD6t8+fL68ccfFRsba1/+9NNPa82aNVq/fr3lbQwePFjLli3Tb7/9puDg4Gy/HzNmjMaOHZtt+ezZsxUaGnp5dwAAAABAgZWcnKy+ffvqzJkzCg8Pd7uuT484Xa5XXnlFn376qVavXu10aJKkkSNHKi4uzv5zQkKC/XtRVhvnSkhLS9Py5ct1yy23KCAgIN/GUWv+y0mthScntRaenNRaeHIWpFp9kZNaC0/OglRrXsg8G80TPh2cIiIi5O/vr2PHjjksP3bsmKKiotzG/vvf/9Yrr7yib7/9Vg0aNHC5XlBQkIKCgrItDwgI8PkDdamc1nOl43yRsyDV6ouc1Fp4clJr4clJrYUnZ0Gq1Rc5qbXw5CxIteYmb/L79OIQgYGBatKkicOFHTIv9HDpqXtZvfrqq3rppZe0dOlSXXfddVeiVAAAAABXMZ+fqhcXF6cBAwbouuuuU7NmzfTGG28oKSlJAwcOlCT1799f5cuX1/jx4yVJEyZM0KhRozR79mzFxMTo6NGjkqSiRYuqaNGiPrsfAAAAAAovnw9OvXr10okTJzRq1CgdPXpUjRo10tKlS1W2bFlJ0sGDB+Xn978DY++9955SU1N11113OdzO6NGjNWbMmCtZOgAAAICrhM8HJ0kaOnSohg4d6vR3q1evdvj5wIEDeV8QAAAAAFzC538AFwAAAADyOwYnAAAAALDA4AQAAAAAFhicAAAAAMACgxMAAAAAWGBwAgAAAAALDE4AAAAAYIHBCQAAAAAsMDgBAAAAgAUGJwAAAACwwOAEAAAAABYYnAAAAADAAoMTAAAAAFhgcAIAAAAACwxOAAAAAGCBwQkAAAAALDA4AQAAAIAFBicAAAAAsMDgBAAAAAAWGJwAAAAAwAKDEwAAAABYYHACAAAAAAsMTgAAAABggcEJAAAAACwwOCFfSs8wWr//lDadtGn9/lNKzzC+LsmtnNZb0O4nch89kHeu9LYtSI+lL2q9nJxXeh9bkGq9HFfD9ilItfoiZ0GqNT8o4usCULhd+uQovf+UYquXkb+fzW3M0l+PaOyXv+vImfOS/DVjz0ZFFw/W6C511LFe9JUp3As5rddX9zMnj0lByleQFLRe94Wc9s+V3rYF6bH0Ra2Xk/NK72MLUq2X42rYPgWpVrZP/ttXOsMRp6vIlf5UYemvR3TDhJW6e+pGzdjjr7unbtQNE1Zq6a9H3MY8MnPz/z+h/ufomfN6ZOZmt7GXU2tO43Jab27cz5zIyWNSkPJlKgifoPmqBy5HQdiHZMZdzrb1xWNZkPruSua80vvYglTrpa7UY1KQtk9BqtUXOQtSrfmJzRhTcI6P5YKEhAQVL15cZ86cUXh4uK/LUVpamr766it17txZAQEBeRbnOOFflLNPFTyLzXxyZG2uzM+J37u7cbbYC+kZajlhpY4lpDi9TZukqOLB+n7ETS4/cc7p/cxJXHqG0Q0TVmbbAVjVm9O4rHLSA94+JpeTMzfyeZszM++V7PWcxOVWD0jsQ7K63G3ri8eyIPVdXuVc+3RbpaUbJaVeUHJKus6mXFDi+TQ9PHOzTienutwG4cFF9Pgt18jP9r96M4zRpOW7lXD+gsu4iKKBmnlfcxULCVDRwCIKDfKXn83mca0XMoySU9OVlHJBSakXlHDugh6euUmnknK3Vl8+Jrn52uVp7Kon2yglLUNnUy8oOeWCEs5f0AMzNub6dnUVdzmxBSlnfqzVm9e93ObNbMDg5EPpGUbr9h7XN2vXq/2NzT0+BcXbuMt5A5uTWKsdpCQVCyqiro3K6eTZFJ1ITNGJsyk6Gn9eaR58ytqoYnFdW6mkYkqHKSYiTDGlQ1W+RIi+3XEsR/czJ/fxXGq6Fm89pBGfb7est265cBUP+d8b0zPn0vTb4QTLuDkPXK/YaqWd/s7bHsiNN0ze5MytwSC/97o3cekZRv+cTdHxxBSt3XNCE5bucnk/MrnrAenq3oeUCgvUuG71dP5ChpJSL1x8A5uSrr3HE7Vk+1GXcZleubO+2tUpq1KhgfL7//vu1WOZdHHf9d1uzx7LGYOaqtU1ZbItz6u+m9znWtUpF64DJ5O0/2SS/vwnWVv+Oq3th6z3PQ0qFFejiiUUUzpMVSLCVLl0qCqWCtUKD/exF9Iz9E9Sqo4npGjtnuN6ddluy5z5QRE/my548BrkZ5Ou9FcymsWUVJOYUqpS+uLjUSUiTJHFgrTst6MePSbJqRd0IvHi/ueHvSf1xrd7LHPm9LUra5w3sYDV615eYHByI78MTvnlE0ZJKhESoOdure30U4V/LdmhM+fSXMaGBvrr5lplLn7ylnpByanpOpmYosNu8uWFIn6Skc3t6QmRxYK0eEhLFQsJUGiAv/z8bB5tn5KhAXqgVVUd/CdZB/5J0oGTyTqakPf3r1ujchp0QxXViQ5XEf//nVXrbQ+cSEzR/E1/efTmbtb9zdWyekS25d7kzMgwWrz1kB6bu9Uyn7sdZH7rdWexnsQV8bOpeEiATienev1m6+baZdS3WSVdV7mUioc6vhEpTPuQW2qXUXJahpJTL+hsSrpOJp7Xofgrsw/x97MpomigIooGau/xJKVcyHC5bhE/m0qEBuhUkvePpU1S+ZIh//+BT6hiSoepUslQPbtwu06edf5p+qUfMEiyH41JOJ+mPlN+0j8u4vKCn02yyaZ0N28ZivjZFB5cRKfPpely3lmEBvorLKiIjDEut82lGlUsofIlQ+w/Hzp9Tlv+ireMCwv0V1qGUaqbx9xKSIC/woL8ZYz0j5ujIpdbq/PcfkpLN26HPX8/m4KL+CkpNT1HOXwl0N9PYUH+kqTTya73H5lyul2zxl1ObEHKmZ9rfbN3I93eqLzlermJwcmN/DA45dUnjO/0baz6FYr//5v7JO0/mawtB09rcw53ynmtQ52yalkjQmWKBSmyWJAOnT6nYZ9usYy774YY+fv5/f+nqEk68E9yjl74QgL8FeBvc3u42Z3QAD8lp1nnHdq2mmqULWb/ec+xRL29ap/HecIC/dW4ckk1iyklI2nS8t0ue+DF2+uqdNEg/Xb4jH47nKDfDifoRKLzUx+dCfS3qV754qpbrrjqlgu3f2I9/NMtLnM+27m2SoQG6LfDCfr9cIJ2HElQYopn27RkaIDa1CyjpjGl1KxKSVWLLCqbzebxc8QYo8Nnzuu3Q2e07Lej+nzzIY/vqy/YbFLpsCCFBfrrz1PJXsXWLFtMzaqUUtMqpZSUckHPfrE91/ch7/ZrrMaVS+rAySQd+OfiPmTzn6e04cBpr2q9UiqXDlXFkqH2N9qhgf5KOJemL7dZnycfHlxEiSkXcvwG/3IeS28F+NuUlu59oUFF/FQ1sqiqRISqcukwZWQYffDdH5ZxD9xYRf5+fvY++POfZJ1L8+6Nd+ZAGhLorwMnrbfPe3c3VqsakQr5/w+1JGndvn/UZ8pPlrFZP4DxNi4tPUPJKelau+eEhs75xTLu3X6NdWONCIUGFrEfqc3rWu+5vrIkXXx9/ydJh06f83p4Dw7wU5liwQoO8NPuY2ct18/pa1fWOG9i/9P/OrW6JlKBRS5+WHileuBSV0POglTrleDNbMBV9a6w9AyjsV/+nu2NiyT7spFfbJcxsr94SBc/xX92QfY3SpfGDZ69Ocd11Y4qpjLhwQ7Ljiec146jiZaxd15bXs2rllJoYBGFBflr/8kkvfTfHZZx97as4vDkaFTRaPzXO3X0zHmn9zPz09dnO9dxOK0oI8Noxk8HNGbx75Y5bfrf9jqXli43H4Q7aFyphG6oEamY0qGKiQhTldJhKhZcRDe+usqy3sdvqZntPPHPNx9yGSdJxYKLqEmlEtp0MF6J5y9o7Z6TWrvnpMv6Mm/nhUW/Za/DJkWFB7s9YpApNd1o88F4bT4Yb7luZs6Xv8r+WHt6ysvp5DQt+OWQFvxyceApGRqgJpVLav3+U257/Yl5WzVj3QH9fiRR8R58Gnmpy+n1rLGexj3VoaZ6XFdBpUIDVcTfz34Ux10PlAgJ0C11ymrTn6f1x8kk7TqWqF3HEvXJT3+6zHO5+5BHZvlmH9KtUTk1q1JaYUH+Cg0soj//SdK/lljvQ165s0G2F9j0DKONf562fF5+P+ImZRijU/9/StmX2w7pw+/2W+Z8qkNN9WhSQaXCPHssM/N9MbiF/j597n8f+JxM1pa/4nUo/pxlzkuHJn8/mwL9bTrnwYc2r3ZvoNuv/d8nt+kZRou3Hras9ZlOtR32WcYYfbLuT41anH3/ktXTHWqqZ9OKKhkaKP9Ljupb5WxfJyrb6aLNqpRSdPFgy9hmVUpdVlyAv5+Kh/qpU/1oRX+1wzKuQ90rX+uYrnUdcqZcSNeMH/90uv/N6rnOtdW7WUUVDSoim83zx8Tb1y5Xcd7Etq3leNrwleqB3IgtSDkLUq35DYPTFbZh/ynLN7Cnk9Ny/AamiJ/t/7/3E6YqEaFKzzCa+sMBy7hRXerm+FOFHtdVdIhNzzD6z9r9Xj85/P1sGt2ljh6ZudlhwMmMkaTRXepk2yH7+dlUs6xnRw9nP9Bc11YqqaSUi6cV/rjvpEffU3qqQy2nn4DkpF5P7udrdzWwf5di19FE/XzglL7+9Yh++uOUZa1VIsLUvEop+xGjWlHhCg7w9+iFcvrAptp5NPH/j1ad0Za/4pWUYv1Jc+2oYoqtFqE65cJVt1y4qkSEqe2/V7vNVzY8WBO619emP09rw4FT+uVgvE4np+nbHcct8yWlpOvHfRe3RRE/m6qXKaoyxYL0nZvhMtPl9HrWWE/jGlcqqTLF/jdUeNIDr3Svbz9qdCIxRRsPnNKGA6e0atdxy0/wL2cfYpNUsVTmBwShMkaa4WZYy3Q527VX00rZ9iEffe/9PkTybj/iL5vKhgerbHiwzqZc8GhwalyppMOA6Gm+6OIhii4eoqYx/6vZ0+0zuXcj3VAjUqGB/goq4qef/jjlUVzWQTan+1ibzZbtCIIr11YqqYiiQZed83Jir3ScL3IGFfFXvfLFs9XhTL3yxVUs+H+n+l4N26cg1eqLnAWp1vyGy5FfYccTPTtvv0pEmJpULmn/VyUizKO413o01LdxrfWfAdfpuVvr6Llb6yi6eLBctaFNF7/f4O5TBW9jM58cmetkjZFcPzk61ovWe3c3VlRxxxf8qOLBbr+A7nmtpRUc4K/SRYNUsVSo7mpSMcfb53Lq9TTO38+mOuXCNaBFjPo0q+SiSkePtauhV7o30D2xMWpSuZTCgop4/JjUjArX7Y3K69nOtTXr/us1rlt9j3I+3KaaRnWpo7uaVFDt6IuDmlW+MV3rqHXNMoprX1OfPhir7WM66PNHWqhrw3Ie5ezTrKK+HHqDfh3bQUsfa6VpA5td8V7PaZzkXe9EFgtSp/rRGt2lrh5vd42LbI5yug95vUdDffd0W80Y1Exjb6+n0V3rFqh9iJSz5+WVeixzkvPWBuVUKixQwQH+stls+brW3Mx5ObFXOs4XOX3xmBSk7VOQavVFzoJUa37Cd5yuMF+cG5r5vQbJ+YTvyRWxchqb08sX5+RqYTmt9XLu4+XU621cbpwf7O1jcrk5c9IDBa3XL7d/vO0d9iGe/6HEnF49MCe15iTf5eQsSLVeTs7Ljb3ScVc6p68ek4KyfQparb7IWZBqzSvezAYccbrC8vsn27kd+/2ImzRz0HXqXyNdMwddp+9H3OTRGx5/P5uaVymlJhFGzauU8ugJ5ctPQHJSr7dxl9MDmbx9TC43Z056oKD1+uX2j7e9wz7Es32I5P22vdKP5eXkLEi1Xk7Oy4290nFXOqevHpOCsn0KWq2+yFmQas0P+I7TFearc0M71ovWLXWicjThX05s5pPjnx1X5smR01ov5z5eKbl1frA3j0lu5PS2Bwpir1/J/mEfkj/3Ib7IWZBqRd7hMQGuHI44+UBB+oQxN2KvtML8CYgvzg8uiDkL+ydo7EPyli9qLQh958uccI/HBLgyOOLkIwXpE0bkL1fLp8z0unvsQwAAuLIYnHwop6egXOlTV5D/+KIHrpacBQn7EAAArhxO1QMAAAAACwxOAAAAAGCBwQkAAAAALDA4AQAAAIAFBicAAAAAsMDgBAAAAAAWGJwAAAAAwAKDEwAAAABYYHACAAAAAAsMTgAAAABggcEJAAAAACwwOAEAAACABQYnAAAAALDA4AQAAAAAFhicAAAAAMACgxMAAAAAWGBwAgAAAAALDE4AAAAAYIHBCQAAAAAsMDgBAAAAgAUGJwAAAACwwOAEAAAAABYYnAAAAADAAoMTAAAAAFhgcAIAAAAACwxOAAAAAGAhXwxO77zzjmJiYhQcHKzmzZtrw4YNbtefN2+eatWqpeDgYNWvX19fffXVFaoUAAAAwNXI54PT3LlzFRcXp9GjR2vz5s1q2LChOnTooOPHjztd/8cff1SfPn1033336ZdfflG3bt3UrVs3/frrr1e4cgAAAABXC58PThMnTtQDDzyggQMHqk6dOnr//fcVGhqqqVOnOl3/zTffVMeOHfXUU0+pdu3aeumll9S4cWO9/fbbV7hyAAAAAFeLIr5Mnpqaqk2bNmnkyJH2ZX5+fmrXrp3WrVvnNGbdunWKi4tzWNahQwctXLjQ6fopKSlKSUmx/3zmzBlJ0qlTp5SWlnaZ9+DypaWlKTk5Wf/8848CAgLybRy15r+c1Fp4clJr4clJrYUnZ0Gq1Rc5qbXw5CxIteaFxMRESZIxxnJdnw5OJ0+eVHp6usqWLeuwvGzZstq5c6fTmKNHjzpd/+jRo07XHz9+vMaOHZtteZUqVXJYNQAAAIDCJDExUcWLF3e7jk8Hpyth5MiRDkeoMjIydOrUKZUuXVo2m82HlV2UkJCgihUr6q+//lJ4eHi+jaPW/JeTWgtPTmotPDmptfDkLEi1+iIntRaenAWp1rxgjFFiYqLKlStnua5PB6eIiAj5+/vr2LFjDsuPHTumqKgopzFRUVFerR8UFKSgoCCHZSVKlMh50XkkPDw8R41zpeN8kbMg1eqLnNRaeHJSa+HJSa2FJ2dBqtUXOam18OQsSLXmNqsjTZl8enGIwMBANWnSRCtWrLAvy8jI0IoVKxQbG+s0JjY21mF9SVq+fLnL9QEAAADgcvn8VL24uDgNGDBA1113nZo1a6Y33nhDSUlJGjhwoCSpf//+Kl++vMaPHy9JGj58uFq3bq3XX39dt956qz799FNt3LhRH374oS/vBgAAAIBCzOeDU69evXTixAmNGjVKR48eVaNGjbR06VL7BSAOHjwoP7//HRhr0aKFZs+ereeff17PPvusatSooYULF6pevXq+uguXJSgoSKNHj852OmF+i/NFzoJUqy9yUmvhyUmthScntRaenAWpVl/kpNbCk7Mg1eprNuPJtfcAAAAA4Crm8z+ACwAAAAD5HYMTAAAAAFhgcAIAAAAACwxOAAAAAGCBwclHvvvuO3Xp0kXlypWTzWbTwoULPYobP368mjZtqmLFiqlMmTLq1q2bdu3aZRn33nvvqUGDBvY/NBYbG6uvv/7a67pfeeUV2Ww2PfbYY5brjhkzRjabzeFfrVq1PMpz6NAh3X333SpdurRCQkJUv359bdy40TIuJiYmW06bzaYhQ4a4jUtPT9cLL7ygKlWqKCQkRNWqVdNLL70kT66dkpiYqMcee0yVK1dWSEiIWrRooZ9//jnbelaPuTFGo0aNUnR0tEJCQtSuXTvt2bPHMu6LL75Q+/btVbp0adlsNm3ZssUyX1pamkaMGKH69esrLCxM5cqVU//+/XX48GGPah0zZoxq1aqlsLAwlSxZUu3atdP69eu96uuHH35YNptNb7zxhkc577333myPa8eOHT3KuWPHDnXt2lXFixdXWFiYmjZtqnnz5rmNc9ZHNptNr732mmXOs2fPaujQoapQoYJCQkJUp04dvf/++5Zxx44d07333qty5copNDRUHTt21J49ezx63p8/f15DhgxR6dKlVbRoUXXv3l3PPfecZdyHH36oNm3aKDw8XDabTfHx8ZKs9zWnTp3So48+qpo1ayokJESVKlXSsGHDNHr0aMucDz30kKpVq6aQkBBFRkbq9ttvV1xcnMf7NmOMOnXqZN+GnmyfNm3aZHssmzdv7lHOdevW6aabblJYWJjCw8PVqlUrvfjii25jDxw44LKHqlev7jbn0aNHdc899ygqKkphYWFq3Lix+vXrZ1nrvn37dMcddygyMlLh4eHq2bOnjh07Zrn/d9Y7nsS56h3J/WuOq945c+aMZU5nvbNz506PX+Oy9o5Vra565+GHH/Yop7PeefPNN13GueubefPmWeZ01juff/65ZZyr3snK2XsAV/3jSay7HnIV565/rPK56h9Pas3krIfcxbnqH0/yOeufc+fOuY216iF3OV31jyfbx1kPPfnkk27fB3raO/kJg5OPJCUlqWHDhnrnnXe8iluzZo2GDBmin376ScuXL1daWprat2+vpKQkt3EVKlTQK6+8ok2bNmnjxo266aabdPvtt+u3337zOPfPP/+sDz74QA0aNPA4pm7dujpy5Ij93/fff28Zc/r0abVs2VIBAQH6+uuv9fvvv+v1119XyZIlParx0nzLly+XJPXo0cNt3IQJE/Tee+/p7bff1o4dOzRhwgS9+uqreuuttyxz3n///Vq+fLk++eQTbd++Xe3bt1e7du106NAhh/WsHvNXX31VkydP1vvvv6/169crLCxMHTp00OnTp93GJSUl6YYbbtCECRM8zpecnKzNmzfrhRde0ObNm/XFF19o165d6tq1q0e1XnPNNXr77be1fft2ff/994qJiVH79u11+PBhj/p6wYIF+umnn1SuXDmPt48kdezY0eHxnTNnjmXcvn37dMMNN6hWrVpavXq1tm3bphdeeEEXLlxwG3dpniNHjmjq1Kmy2Wzq3r27Zc64uDgtXbpUM2fO1I4dO/TYY49p6NChWrFihcs4Y4y6deumP/74Q4sWLdIvv/yiypUrq127dlq5cqXl8/7xxx/Xl19+qXnz5mnNmjU6fPiw3n33Xcu45ORkdezYUc8++6xDPVb7msOHD+vw4cP697//rV9//VXTp0/X0qVL9cEHH1jmbNKkiaZNm6YdO3Zo2bJlMsbovffe0yOPPOLRvu2NN96QzWbzuNZMDzzwgMNjWqxYMcu4devWqWPHjmrfvr02bNign3/+WUOHDtX333/vNrZixYrZemjs2LHy9/fXk08+6TZn//79tWvXLi1evFjbt2/XnXfeqdmzZ+u2225zGZeUlKT27dvLZrNp5cqV+uGHH5Sammof1N3t/531zp133mn5uuGqdyT3rzmueue+++6zzOmsd9q3b6/o6GiPXuOy9o5Vra5659VXX7WMc9U7FStWdBnnqm+KFi2qTp06WeZ01js9e/ZUSkqKyzh3vZORkWHfBq7eA7jqn0u5inXXQ67i3PWPVT5X/ZOenm4Zm8lZD1nFOesfqzhX/XPpn+hxFmvVQ+5yuuqfX375xW1OVz00Z84ct+8DPemdfMfA5ySZBQsW5Cj2+PHjRpJZs2aN17ElS5Y0//nPfzxaNzEx0dSoUcMsX77ctG7d2gwfPtwyZvTo0aZhw4Ze1zVixAhzww03eB3nzPDhw021atVMRkaG2/VuvfVWM2jQIIdld955p+nXr5/buOTkZOPv72/++9//Oixv3Lixee6551zGZX3MMzIyTFRUlHnttdfsy+Lj401QUJCZM2eOy7hL7d+/30gyv/zyi2U+ZzZs2GAkmT///NPr2DNnzhhJ5ttvv7WM+/vvv0358uXNr7/+aipXrmwmTZrkUb0DBgwwt99+u9s6nMX16tXL3H333V7HZXX77bebm266yaPYunXrmhdffNFhWdaeyBq3a9cuI8n8+uuv9mXp6ekmMjLSTJkyxeG2sj7v4+PjTUBAgJk3b559nR07dhhJZt26dS7jLrVq1SojyZw+fdrp/fdkX/PZZ5+ZwMBAk5aW5lXc1q1bjSSzd+9ey7hffvnFlC9f3hw5csTl4+Ys1pP9lrO45s2bm+eff95tnLt6L9WoUaNs+xlncWFhYWbGjBkO65UqVcqhD7LGLVu2zPj5+ZkzZ87Y14mPjzc2m80sX748Wy2Z+39Peydr3KWsesddbCZnveNJnLPecRXnSe84i/X0NS9rnKe946zWSznrG1exnvRO1jhPesfVewBP+seT9w/Oesib9x2X9o83cVn7xyrWVQ+5i3NXg7s4q/7x5n5e2kPu4qz6x1Wsqx6SZKpWreq0Jm/3PfkFR5wKuMxD06VKlfI4Jj09XZ9++qmSkpIUGxvrUcyQIUN06623ql27dl7Vt2fPHpUrV05Vq1ZVv379dPDgQcuYxYsX67rrrlOPHj1UpkwZXXvttZoyZYpXeSUpNTVVM2fO1KBBg5x+OnSpFi1aaMWKFdq9e7ckaevWrfr+++/tn864cuHCBaWnpys4ONhheUhIiEdH1zLt379fR48eddi+xYsXV/PmzbVu3TqPb+dynDlzRjabTSVKlPAqLjU1VR9++KGKFy+uhg0bul03IyND99xzj5566inVrVvX6xpXr16tMmXKqGbNmnrkkUf0zz//WOZbsmSJrrnmGnXo0EFlypRR8+bNPT41NtOxY8e0ZMkSh0803WnRooUWL16sQ4cOyRijVatWaffu3Wrfvr3LmJSUFEly6CU/Pz8FBQVl66Wsz/tNmzYpLS3NoX9q1aqlSpUqOfRPTvYX3sSeOXNG4eHhKlKkiMMyd3FJSUmaNm2aqlSpoooVK7qNS05OVt++ffXOO+8oKirK61pnzZqliIgI1atXTyNHjlRycrLbuOPHj2v9+vUqU6aMWrRoobJly6p169ZOn9tW93PTpk3asmVLth5yFteiRQvNnTtXp06dUkZGhj799FOdP39ebdq0cRmXkpIim83m8Mckg4OD5efn51Bv1v2/p72Tk9cNb2Kd9Y5VnKvecRbnae+4ymnVO1njPO0dq/voqm9cxXrSO1njPOkdV+8BPOmfnL5/8Cbu0v7xNM5Z/7iLdddDVjld9Y+rOE/6x9P7mbWH3MVZ9Y+rWFc9ZLPZ9Ndffzl9H+jpviff8fXkhpwfcUpPTze33nqradmypUfrb9u2zYSFhRl/f39TvHhxs2TJEo/i5syZY+rVq2fOnTtnjPH807evvvrKfPbZZ2br1q1m6dKlJjY21lSqVMkkJCS4jQsKCjJBQUFm5MiRZvPmzeaDDz4wwcHBZvr06R7Vm2nu3LnG39/fHDp0yHLd9PR0M2LECGOz2UyRIkWMzWYz48aN8yhPbGysad26tTl06JC5cOGC+eSTT4yfn5+55pprXMZkfcx/+OEHI8kcPnzYYb0ePXqYnj17uoy71OUccTp37pxp3Lix6du3r8exX375pQkLCzM2m82UK1fObNiwwTJu3Lhx5pZbbrEfAfTmiNOcOXPMokWLzLZt28yCBQtM7dq1TdOmTc2FCxdcxmV+KhgaGmomTpxofvnlFzN+/Hhjs9nM6tWrLe9jpgkTJpiSJUvanwNWtZ4/f97079/fSDJFihQxgYGB5uOPP3Ybl5qaaipVqmR69OhhTp06ZVJSUswrr7xiJJn27dvb13P2vJ81a5YJDAzMVlvTpk3N008/7TLuUu6OGniyrzlx4oSpVKmSefbZZz2Ke+edd0xYWJiRZGrWrOlwxMBV3IMPPmjuu+8++8/Otr2r2A8++MAsXbrUbNu2zcycOdOUL1/e3HHHHW7j1q1bZySZUqVKmalTp5rNmzebxx57zAQGBprdu3d7tX0eeeQRU7t2bY9qPX36tGnfvr29f8LDw82yZcvcxh0/ftyEh4eb4cOHm6SkJHP27FkzdOhQI8k8+OCDLvf/Vr3jyeuGq97x9DUna+9YxbnqHXdxVr3jLtZd77iKs+odT7eNs75xF+uud1zFWfWOu/cAVv3j6fuHrD3kzfuOS/vHkzhX/WMV66qHrOJc9Y+7OKv+8Wb7XNpDVnHu+sddrLseateundP3gZ68buVHDE75QE4Hp4cffthUrlzZ/PXXXx6tn5KSYvbs2WM2btxonnnmGRMREWF+++03tzEHDx40ZcqUMVu3brUv8+a0hUudPn3ahIeHW54eGBAQYGJjYx2WPfroo+b666/3Kl/79u3Nbbfd5tG6c+bMMRUqVDBz5swx27ZtMzNmzDClSpXyaFjbu3evadWqlZFk/P39TdOmTU2/fv1MrVq1XMbkp8EpNTXVdOnSxVx77bUOh9mtYs+ePWv27Nlj1q1bZwYNGmRiYmLMsWPHXMZt3LjRlC1b1mGQ9WZwymrfvn2WpwceOnTISDJ9+vRxiO3SpYvp3bu3x/lq1qxphg4d6vR3zmJfe+01c80115jFixebrVu3mrfeessULVrU4ZQpZ3EbN240DRs2tPdShw4dTKdOnUzHjh3t6zh73nvyAmS1v3A3OFnFnjlzxjRr1sx07NjRpKamehQXHx9vdu/ebdasWWO6dOliGjdubH9Bdha3aNEiU716dZOYmGhf5mwberpfXLFihcMpOs7iMp+XI0eOdIitX7++eeaZZzzOmZycbIoXL27+/e9/e1Tr0KFDTbNmzcy3335rtmzZYsaMGWOKFy9utm3b5jZu2bJlpmrVqsZmsxl/f39z9913m8aNG5uHH37Y5f7fqnc8ed1w1TuexDrrHas4V73jKs6T3vHm9fHS3nEVZ9U7nuRz1TfuYt31jrs4V71z9913u30P4K5/Hn74YY/fP1zaQ96877i0f/bt2+dRnLP+2b17t9tYVz00ZcoUr98jZfZP6dKlXca565/Bgwd7nPPSHvJku7rqn2+++cYy1t3+J9Ol7wMZnJBjORmchgwZYipUqGD++OOPHOe9+eabzYMPPuh2nQULFtjfxGX+k2R/Ylz6ab8nrrvuOoc3HM5UqlTJ4VMdY4x59913Tbly5TzOc+DAAePn52cWLlzo0foVKlQwb7/9tsOyl156ydSsWdPjnGfPnrUPPj179jSdO3d2uW7WxzxzCMg69LRq1coMGzbMZdylcjI4paammm7dupkGDRqYkydPelSrK9WrV3c4Spc1btKkSfa+ubSX/Pz8TOXKlXOUMyIiwrz//vsu41JSUkyRIkXMSy+95BD39NNPmxYtWniU77vvvjOSzJYtW5z+PmtscnKyCQgIyPa9t/vuu8906NDBo5zx8fHm+PHjxhhjmjVrZgYPHmyMcf28z3whzvrGtVKlSmbixIke7S9cvfm1ik1ISDCxsbHm5ptvdjgi580+KiUlxYSGhprZs2e7jBs+fLjL/mndurXXOc+ePWskmaVLl7qM++OPP4wk88knnzgs79mzp/3orCc5Z8yYYQICAuyPqbu4vXv3ZvuumzEX99cPPfSQR/lOnDhhfxzLli1rXn311WzrZO7/rXrHVdylPP2OU9ZYV73jSc5Ml/aOqzhPesebnJf2jqs4T3rHKp+zvnFXq1XveJIza+9kHjl39R7g22+/ddk/AwcO9Pj9w6U95On7jqz9k5P3K5n98/jjj7uNHTp0qNMestlsXufM7B93cZmPpbP+ufHGGz3OeWkPWW0fd/2TeRTKk5xW+5/M94He7nvyi/+dTIwCwRijRx99VAsWLNDq1atVpUqVHN9WRkaG/XsVrtx8883avn27w7KBAweqVq1aGjFihPz9/T3Od/bsWe3bt0/33HOP2/VatmyZ7TK7u3fvVuXKlT3ONW3aNJUpU0a33nqrR+snJyc7XKlGkvz9/R2uKGQlLCxMYWFhOn36tJYtW+Zw1RwrVapUUVRUlFasWKFGjRpJkhISErR+/Xo98sgjHt+ON9LS0tSzZ0/t2bNHq1atUunSpS/r9qz66Z577sl2XnSHDh10zz33aODAgV7n+/vvv/XPP/8oOjra5TqBgYFq2rTpZfXTRx99pCZNmlh+fytTWlqa0tLSLqufihcvLunidwQ3btyoF198UUOHDnX5vG/SpIkCAgK0YsUKde/eXZK0a9cuHTx4UD/88IPWrVvn9f7Ck31NQkKCOnTooKCgIC1evFjBwcE52kcZY5SRkaH//Oc/2rlzp9O4Z555Rvfff7/Dsvr162vSpEm67bbb3G4fZzIv2z9t2jStXbvWaVxMTIzKlSvntH86duzocc6PPvpIXbt2VWRkpOX2yfzuQ9b+8fPz09q1axUfH2+ZLyIiQpK0cuVKHT9+3H61zEtlPl/d9Y6z79x48rrhyqWxznrHk7iszMUPgJ3+PjNu7NixLnunS5cuXufM7B1n+57MOHe94+y7s87yXdo37mTGuuodV/seZzmz9s6wYcP01FNPOaxz6XuAihUruuyffv36KS4uzmWsq/cPnrzvcNY/OXm/ktk/NWrUcBsbERGhhx56yOH39evX1yuvvKIGDRqoQoUKHufM7J/58+erZs2aTuOqVq3qsn9uuukmvfvuux7dz0t7yGr7uOuf6Ohoj7etu/3Ppe8Dvd335BcMTj5y9uxZ7d271/7z/v37tWXLFpUqVUqVKlVyGTdkyBDNnj1bixYtUrFixXT06FFJF99ohYSEuIwbOXKkOnXqpEqVKikxMVGzZ8/W6tWrtWzZMrd1FitWTPXq1XNYFhYWptKlS2dbntWTTz6pLl26qHLlyjp8+LBGjx4tf39/9enTx23c448/rhYtWmjcuHHq2bOnNmzYoA8//FAffvih27hMGRkZmjZtmgYMGODwRWN3unTpopdfflmVKlVS3bp19csvv2jixIkaNGiQZWzmJU1r1qypvXv36qmnnlKtWrWyDQNWj/ljjz2mf/3rX6pRo4aqVKmiF154QeXKlVO7du3sO1pncadOndLBgwftf4Mpc0dbrFgxJSYmOo2Ljo7WXXfdpc2bN+u///2v0tPT7b1UqlQppaamuqy1dOnSevnll9W1a1dFR0fr5MmTeuedd3To0CHdeuutbmvNOpwFBAQoKipKNWvWdLt9SpUqpbFjx6p79+6KiorSvn379PTTT6t69epq2bKl25xPPfWUevXqpVatWqlt27ZaunSpvvzyS3311Vdu46SLb+7mzZun119/3avHsnXr1nrqqacUEhKiypUra82aNZoxY4bGjRvnNue8efMUGRmpSpUqafv27Ro+fLi6deumRYsWuX3eFy9eXPfdd5/i4uJUqlQphYeH69FHH1XZsmX17bffut1fHD16VEePHrXfn+3bt6tYsWKaPHmyvvjiC5exCQkJat++vZKTkzVz5kwlJCQoISFBzzzzjBYuXOgy7o8//tDcuXPVvn17RUZG6u+//9Yrr7wiY4w2btyoxYsXO42Liopy+qX+SpUq6d///rfb7bNv3z7Nnj1bnTt3VunSpbVt2zY9/vjjio6O1tKlS13G2Ww2PfXUUxo9erQaNmyoRo0a6eOPP9bOnTtVp04dLVmyxHJfvHfvXn333Xf66quvJFnvw2vVqqXq1avroYce0r///W+VLl1aCxcu1PLlyxUWFqYlS5a4zDdt2jTVrl1bkZGRWrdunYYPH67HH39c06dPd7n/d9U7sbGxWrRokVJTU12+brjqnUqVKum1115zmdNV70jSpEmTdOuttzqNc9U7ISEh2rx5s6pWreo0zl3vVKlSxe3ro6veadWqlebMmaP4+Hince56p3nz5vruu+/cvh5n7ZtM7mp11zt33XWX25yueqdJkybZtlvW9wCu+ufmm2+2jHXXQ5deMOXSOHf9U7t2bYc38ZfGueuf7t27q0yZMm5rddZD11xzjTp27Ogyzl3/ZA4LrvK56p/58+erWrVqbmOl7D1k9X4uLS3NZf/897//tXwv6KyHGjdurKNHjyooKCjb+0B3+57rr78+27bON3x0pOuql3lYOuu/AQMGuI1zFiPJTJs2zW3coEGDTOXKlU1gYKCJjIw0N998s/nmm29yVLun33Hq1auXiY6ONoGBgaZ8+fKmV69eTi8Z68yXX35p6tWrZ4KCgkytWrXMhx9+6HF9y5YtM5LMrl27PI5JSEgww4cPN5UqVTLBwcGmatWq5rnnnjMpKSmWsXPnzjVVq1Y1gYGBJioqygwZMsTEx8dnW8/qMc/IyDAvvPCCKVu2rAkKCjI333yz2bVrl2XctGnTXP7e1fLM0/qc/Vu1apXbnOfOnTN33HGHKVeunAkMDDTR0dGma9euZsOGDV739aXfcXIXm5ycbNq3b28iIyNNQECAqVy5snnggQfM0aNHPcr50UcfmerVq5vg4GDTsGFDs3DhQo/iPvjgAxMSEpLt8bSKPXLkiLn33ntNuXLlTHBwsKlZs6Z5/fXXzcqVK93Gvfnmm6ZChQomICDAVKpUyTz//PMmJSXFo+f9uXPnzODBg03JkiVNaGioueOOOzyKGz16tMv13MW62gZWcYcOHTKdOnUyZcqUMQEBAaZChQqmb9++Odq3SbKffuIu9uDBg6ZVq1amVKlSJigoyFSvXt089dRTHuccP368qVChggkNDTWxsbFm7dq1HseOHDnSVKxY0aSnp9trtorbvXu3ufPOO02ZMmVMaGioadCggUdxI0aMMGXLljUBAQGmRo0a5vXXXzcZGRmW+39nvXPkyBHLOFe9M23aNLex7nqnZ8+eLuNc9c7OnTu9fo3L7B1j3L8+uuqdM2fOeJTTWe94Epe1bzJZxTrrnRkzZljGueodZ7K+B3DVP57EuushV3Hu+mf//v0u49z1jye1ZnVpD7mKc9c/nuRz1j+e1uqqh9zFueofT2Kd9ZDV+0Bveie/sBljjAAAAAAALvF3nAAAAADAAoMTAAAAAFhgcAIAAAAACwxOAAAAAGCBwQkAAAAALDA4AQAAAIAFBicAAAAAsMDgBAAAAAAWGJwAAPCCzWbTwoULfV0GAOAKY3ACABQY9957r2w2W7Z/HTt29HVpAIBCroivCwAAwBsdO3bUtGnTHJYFBQX5qBoAwNWCI04AgAIlKChIUVFRDv9Kliwp6eJpdO+99546deqkkJAQVa1aVfPnz3eI3759u2666SaFhISodOnSevDBB3X27FmHdaZOnaq6desqKChI0dHRGjp0qMPvT548qTvuuEOhoaGqUaOGFi9enLd3GgDgcwxOAIBC5YUXXlD37t21detW9evXT71799aOHTskSUlJSerQoYNKliypn3/+WfPmzdO3337rMBi99957GjJkiB588EFt375dixcvVvXq1R1yjB07Vj179tS2bdvUuXNn9evXT6dOnbqi9xMAcGXZjDHG10UAAOCJe++9VzNnzlRwcLDD8meffVbPPvusbDabHn74Yb333nv2311//fVq3Lix3n33XU2ZMkUjRozQX3/9pbCwMEnSV199pS5duujw4cMqW7asypcvr4EDB+pf//qX0xpsNpuef/55vfTSS5IuDmNFixbV119/zXetAKAQ4ztOAIACpW3btg6DkSSVKlXK/v+xsbEOv4uNjdWWLVskSTt27FDDhg3tQ5MktWzZUhkZGdq1a5dsNpsOHz6sm2++2W0NDRo0sP9/WFiYwsPDdfz48ZzeJQBAAcDgBAAoUMLCwrKdOpdbQkJCPFovICDA4WebzaaMjIy8KAkAkE/wHScAQKHy008/Zfu5du3akqTatWtr69atSkpKsv/+hx9+kJ+fn2rWrKlixYopJiZGK1asuKI1AwDyP444AQAKlJSUFB09etRhWZEiRRQRESFJmjdvnq677jrdcMMNmjVrljZs2KCPPvpIktSvXz+NHj1aAwYM0JgxY3TixAk9+uijuueee1S2bFlJ0pgxY/Twww+rTJky6tSpkxITE/XDDz/o0UcfvbJ3FACQrzA4AQAKlKVLlyo6OtphWc2aNbVz505JF6949+mnn2rw4MGKjo7WnDlzVKdOHUlSaGioli1bpuHDh6tp06YKDQ1V9+7dNXHiRPttDRgwQOfPn9ekSZP05JNPKiIiQnfdddeVu4MAgHyJq+oBAAoNm82mBQsWqFu3br4uBQBQyPAdJwAAAACwwOAEAAAAABb4jhMAoNDg7HMAQF7hiBMAAAAAWGBwAgAAAAALDE4AAAAAYIHBCQAAAAAsMDgBAAAAgAUGJwAAAACwwOAEAAAAABYYnAAAAADAwv8BW/KkNLCNOUEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, EPOCHS + 1), val_accuracies, marker='o')\n",
    "plt.title(\"Validation Accuracy over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.xticks(range(1, EPOCHS + 1))\n",
    "plt.ylim(0, 1)  # Since accuracy is 0.0 to 1.0\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f8f3181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-18T23:44:04.033769Z",
     "iopub.status.busy": "2025-08-18T23:44:04.033215Z",
     "iopub.status.idle": "2025-08-18T23:45:01.038898Z",
     "shell.execute_reply": "2025-08-18T23:45:01.037966Z"
    },
    "papermill": {
     "duration": 57.918707,
     "end_time": "2025-08-18T23:45:01.040161",
     "exception": false,
     "start_time": "2025-08-18T23:44:03.121454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/72 [00:00<?, ?it/s]It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n",
      "Testing: 100%|| 72/72 [00:56<00:00,  1.28it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       cover       0.00      0.00      0.00        57\n",
      "     defense       0.00      0.00      0.00        59\n",
      "       flick       0.00      0.00      0.00        55\n",
      "        hook       0.00      0.00      0.00        55\n",
      "    late_cut       0.00      0.00      0.00        56\n",
      "      lofted       0.00      0.00      0.00        60\n",
      "        pull       0.00      0.00      0.00        55\n",
      "  square_cut       0.10      1.00      0.19        60\n",
      "    straight       0.00      0.00      0.00        59\n",
      "       sweep       0.00      0.00      0.00        59\n",
      "\n",
      "    accuracy                           0.10       575\n",
      "   macro avg       0.01      0.10      0.02       575\n",
      "weighted avg       0.01      0.10      0.02       575\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the final or best checkpoint\n",
    "model.load_state_dict(torch.load(\"best_videomae_cricket.pth\"))  # Use best model\n",
    "model.eval()\n",
    "\n",
    "# Run test evaluation\n",
    "preds, labels = evaluate_test(model, test_loader, CLASSES)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7811551,
     "sourceId": 12388154,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 394001,
     "modelInstanceId": 373154,
     "sourceId": 460974,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 32181.113549,
   "end_time": "2025-08-18T23:45:06.019285",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-08-18T14:48:44.905736",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "18b0f13e2b434464b2bdf6b0b133e32e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6deb6e61a5254fbe8abb3c4261645a2d",
       "placeholder": "",
       "style": "IPY_MODEL_e7b7b7f23017415aa5372df3842503fd",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors:100%"
      }
     },
     "1a171a77c69a4cf981e7d60bd09bc4ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_df4f01232d2f4c9fb036faa208535024",
       "placeholder": "",
       "style": "IPY_MODEL_c585f31794bc47fdb06c840efba0d7ea",
       "tabbable": null,
       "tooltip": null,
       "value": "725/725[00:00&lt;00:00,92.4kB/s]"
      }
     },
     "1d581762eed741d1b430d6664b18e334": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1ebab053ba1e4d3f8a8f4467875606df",
       "max": 725.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a31831e6e68a4861af0604ec42313ba9",
       "tabbable": null,
       "tooltip": null,
       "value": 725.0
      }
     },
     "1ebab053ba1e4d3f8a8f4467875606df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1fb89a08ed724b70b85019129bea4a4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "31e1f372360f4f2aac691685a5f84b4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d6d5129c196f4832af8db5a674b2a7f5",
        "IPY_MODEL_1d581762eed741d1b430d6664b18e334",
        "IPY_MODEL_1a171a77c69a4cf981e7d60bd09bc4ac"
       ],
       "layout": "IPY_MODEL_9590156e371b4858a71b24de8afdb197",
       "tabbable": null,
       "tooltip": null
      }
     },
     "38e424c6a7894719819286729628a9a8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "39995e5619254b638d3fa3ef871e5805": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_d0f1e39e13de494c8c93665f22a52370",
       "max": 271.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8f58f8246a694775a24184a84aecf285",
       "tabbable": null,
       "tooltip": null,
       "value": 271.0
      }
     },
     "3dbdb6bb95e2474badc16bdf81bd60d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_669b985869eb4baa84839130c03bbee1",
       "placeholder": "",
       "style": "IPY_MODEL_1fb89a08ed724b70b85019129bea4a4d",
       "tabbable": null,
       "tooltip": null,
       "value": "271/271[00:00&lt;00:00,36.9kB/s]"
      }
     },
     "4bb213934df8485cb40cf4031728507c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_18b0f13e2b434464b2bdf6b0b133e32e",
        "IPY_MODEL_7b770ee41be44e6c918f778a398a9618",
        "IPY_MODEL_65f7a57478424ef9b25f4d0dad05e328"
       ],
       "layout": "IPY_MODEL_adae1ffb86ce414c982806aadf386a73",
       "tabbable": null,
       "tooltip": null
      }
     },
     "4c463a3b23424c15a44ca7fef3f2b888": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "65f7a57478424ef9b25f4d0dad05e328": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4c463a3b23424c15a44ca7fef3f2b888",
       "placeholder": "",
       "style": "IPY_MODEL_83dfbf54ae1348258865dedca7fb9421",
       "tabbable": null,
       "tooltip": null,
       "value": "377M/377M[00:02&lt;00:00,278MB/s]"
      }
     },
     "669b985869eb4baa84839130c03bbee1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6bc263911bcc4115bdc2e9de20a308de": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6deb6e61a5254fbe8abb3c4261645a2d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7b770ee41be44e6c918f778a398a9618": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_acb2bcd5b215468486b9bd180354811c",
       "max": 376873760.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6bc263911bcc4115bdc2e9de20a308de",
       "tabbable": null,
       "tooltip": null,
       "value": 376873760.0
      }
     },
     "7dc17d0ebb3f4bb8ab0d20c1680a865e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9943ee3bb17f477c8e3728067f7d4379",
       "placeholder": "",
       "style": "IPY_MODEL_f1ab803a46484022b0ca5c7921fd9917",
       "tabbable": null,
       "tooltip": null,
       "value": "preprocessor_config.json:100%"
      }
     },
     "83dfbf54ae1348258865dedca7fb9421": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8f58f8246a694775a24184a84aecf285": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9590156e371b4858a71b24de8afdb197": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "95f0b4cca1e74b14b3e42f904e17d1a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9943ee3bb17f477c8e3728067f7d4379": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a31831e6e68a4861af0604ec42313ba9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "acb2bcd5b215468486b9bd180354811c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "adae1ffb86ce414c982806aadf386a73": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c0da3482143b4e518238ef49729bedc0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c2c9ceda6d144bf5a191bda37ae7d203": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7dc17d0ebb3f4bb8ab0d20c1680a865e",
        "IPY_MODEL_39995e5619254b638d3fa3ef871e5805",
        "IPY_MODEL_3dbdb6bb95e2474badc16bdf81bd60d6"
       ],
       "layout": "IPY_MODEL_95f0b4cca1e74b14b3e42f904e17d1a4",
       "tabbable": null,
       "tooltip": null
      }
     },
     "c585f31794bc47fdb06c840efba0d7ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "d0f1e39e13de494c8c93665f22a52370": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d6d5129c196f4832af8db5a674b2a7f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c0da3482143b4e518238ef49729bedc0",
       "placeholder": "",
       "style": "IPY_MODEL_38e424c6a7894719819286729628a9a8",
       "tabbable": null,
       "tooltip": null,
       "value": "config.json:100%"
      }
     },
     "df4f01232d2f4c9fb036faa208535024": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7b7b7f23017415aa5372df3842503fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f1ab803a46484022b0ca5c7921fd9917": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
